#!/usr/bin/env python3
"""
æ¨¡å‹FLOPså’Œå‚æ•°é‡è®¡ç®—è„šæœ¬
æ ¹æ®æä¾›çš„æ¨¡å‹æ¶æ„ä¿¡æ¯è®¡ç®—æ¯ä¸ªæ¨¡å—çš„è®¡ç®—é‡å’Œå‚æ•°é‡
æ‰€æœ‰æ•°å€¼éƒ½ä»¥ç›´è§‚çš„å•ä½æ˜¾ç¤º

FLOPsè®¡ç®—è¯´æ˜ï¼š
- å¯¹äºçŸ©é˜µä¹˜æ³• A(mÃ—k) Ã— B(kÃ—n)ï¼ŒåŒ…å« mÃ—nÃ—k æ¬¡ä¹˜æ³•å’Œ mÃ—nÃ—(k-1) æ¬¡åŠ æ³•
- æ€»FLOPs = mÃ—nÃ—(2k-1) â‰ˆ 2Ã—mÃ—nÃ—k
- çº¿æ€§å±‚çš„FLOPséƒ½ä¹˜ä»¥2æ¥åŒ…å«ä¹˜æ³•å’ŒåŠ æ³•è¿ç®—
- éçŸ©é˜µä¹˜æ³•è¿ç®—ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ã€softmaxï¼‰ä¸ä¹˜2
"""

def format_number(num):
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸ºæ›´ç›´è§‚çš„å•ä½"""
    if num >= 1e12:
        return f"{num/1e12:.2f}T"
    elif num >= 1e9:
        return f"{num/1e9:.2f}B"
    elif num >= 1e6:
        return f"{num/1e6:.2f}M"
    elif num >= 1e3:
        return f"{num/1e3:.2f}K"
    else:
        return f"{num:.0f}"

def calculate_model_prefill_stats(batch_size=1, seq_len=2048, show_memory=True):
    """
    è®¡ç®—æ¨¡å‹çš„FLOPså’Œå‚æ•°é‡
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        seq_len (int): åºåˆ—é•¿åº¦
        show_memory (bool): æ˜¯å¦æ˜¾ç¤ºå†…å­˜ä¼°ç®—
    """
    
    # æ¨¡å‹åŸºæœ¬å‚æ•°
    vocab_size = 151936
    hidden_size = 4096
    num_layers = 94
    
    # Attentionå‚æ•°
    num_q_heads = 64
    num_kv_heads = 4  # GQA (Grouped Query Attention)
    head_dim = 128
    
    # MoEå‚æ•°
    num_experts = 128
    num_activated_experts = 8
    intermediate_size = 1536  # up projectionçš„ä¸€åŠ
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {format_number(vocab_size)}")
    print(f"  éšè—ç»´åº¦: {format_number(hidden_size)}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  Qå¤´æ•°: {num_q_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"  ä¸“å®¶æ€»æ•°: {num_experts}, æ¿€æ´»ä¸“å®¶æ•°: {num_activated_experts}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {format_number(seq_len)}")
    print("\n" + "="*80)
    
    total_params = 0
    total_flops = 0
    
    # 1. Input Embedding
    print("\n1. Input Embedding Layer")
    embedding_params = vocab_size * hidden_size
    embedding_flops = batch_size * seq_len * hidden_size  # lookupæ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
    
    print(f"  å‚æ•°é‡: {format_number(embedding_params)}")
    print(f"  FLOPs: {format_number(embedding_flops)}")
    
    total_params += embedding_params
    total_flops += embedding_flops
    
    # 2. Attention Layers (x94)
    print(f"\n2. Attention Layers (x{num_layers})")
    
    # Q projection
    q_proj_params = hidden_size * (num_q_heads * head_dim)
    q_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_q_heads * head_dim)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # K, V projection  
    kv_proj_params = hidden_size * (num_kv_heads * head_dim * 2)
    kv_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_kv_heads * head_dim * 2)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Attention computation
    # Q * K^T
    qk_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    # Softmax (è¿‘ä¼¼)
    softmax_flops = batch_size * num_q_heads * seq_len * seq_len * 3  # softmaxä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    # Attention weights * V
    av_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # O projection
    o_proj_params = (num_q_heads * head_dim) * hidden_size
    o_proj_flops = 2 * batch_size * seq_len * (num_q_heads * head_dim) * hidden_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚attentionçš„æ€»è®¡
    attn_params_per_layer = q_proj_params + kv_proj_params + o_proj_params
    attn_flops_per_layer = (q_proj_flops + kv_proj_flops + 
                           qk_flops + softmax_flops + av_flops + o_proj_flops)
    
    total_attn_params = attn_params_per_layer * num_layers
    total_attn_flops = attn_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(attn_params_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_params)}")
    print(f"    - KV projection: {format_number(kv_proj_params)}")
    print(f"    - O projection: {format_number(o_proj_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(attn_flops_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_flops)}")
    print(f"    - KV projection: {format_number(kv_proj_flops)}")
    print(f"    - QK computation: {format_number(qk_flops)}")
    print(f"    - Softmax: {format_number(softmax_flops)}")
    print(f"    - AttentionÃ—V: {format_number(av_flops)}")
    print(f"    - O projection: {format_number(o_proj_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_attn_params)}")
    print(f"  æ€»FLOPs: {format_number(total_attn_flops)}")
    
    total_params += total_attn_params
    total_flops += total_attn_flops
    
    # 3. MoE Layers (x94)
    print(f"\n3. MoE Layers (x{num_layers})")
    
    # Up projection (gate + up)
    up_params_per_expert = hidden_size * (intermediate_size * 2)  # gateå’Œupåˆå¹¶
    total_up_params = up_params_per_expert * num_experts
    activated_up_params = up_params_per_expert * num_activated_experts
    up_flops = 2 * batch_size * seq_len * hidden_size * (intermediate_size * 2) * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Down projection
    down_params_per_expert = intermediate_size * hidden_size
    total_down_params = down_params_per_expert * num_experts
    activated_down_params = down_params_per_expert * num_activated_experts
    down_flops = 2 * batch_size * seq_len * intermediate_size * hidden_size * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¿€æ´»å‡½æ•° (SiLU + ä¹˜æ³•)
    activation_flops = batch_size * seq_len * intermediate_size * num_activated_experts * 3  # æ¿€æ´»å‡½æ•°ä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    
    # è·¯ç”±ç½‘ç»œ
    router_params = hidden_size * num_experts
    router_flops = 2 * batch_size * seq_len * hidden_size * num_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚MoEçš„æ€»è®¡
    moe_params_per_layer = total_up_params + total_down_params + router_params
    moe_flops_per_layer = up_flops + down_flops + activation_flops + router_flops
    
    total_moe_params = moe_params_per_layer * num_layers
    total_moe_flops = moe_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(moe_params_per_layer)}")
    print(f"    - Up projection (æ€»): {format_number(total_up_params)}")
    print(f"    - Down projection (æ€»): {format_number(total_down_params)}")
    print(f"    - Router: {format_number(router_params)}")
    print(f"  æ¯å±‚æ¿€æ´»å‚æ•°é‡: {format_number(activated_up_params + activated_down_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(moe_flops_per_layer)}")
    print(f"    - Up projection: {format_number(up_flops)}")
    print(f"    - Down projection: {format_number(down_flops)}")
    print(f"    - Activation: {format_number(activation_flops)}")
    print(f"    - Router: {format_number(router_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_moe_params)}")
    print(f"  æ€»FLOPs: {format_number(total_moe_flops)}")
    
    total_params += total_moe_params
    total_flops += total_moe_flops
    
    # 4. Output Embedding (é€šå¸¸ä¸input embeddingå…±äº«æƒé‡)
    print(f"\n4. Output Embedding Layer")
    output_embedding_params = 0  # é€šå¸¸å…±äº«æƒé‡
    output_embedding_flops = 2 * batch_size * seq_len * hidden_size * vocab_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    print(f"  å‚æ•°é‡: {format_number(output_embedding_params)} (å…±äº«æƒé‡)")
    print(f"  FLOPs: {format_number(output_embedding_flops)}")
    
    total_flops += output_embedding_flops
    
    # 5. Layer Norm (ä¼°ç®—)
    print(f"\n5. Layer Normalization")
    # æ¯å±‚æœ‰2ä¸ªLayerNorm: attentionåå’ŒMoEå
    layernorm_params = hidden_size * 2 * num_layers * 2  # scaleå’Œbias
    layernorm_flops = batch_size * seq_len * hidden_size * 5 * 2 * num_layers  # æ¯ä¸ªLNçº¦5ä¸ªæ“ä½œ
    
    print(f"  å‚æ•°é‡: {format_number(layernorm_params)}")
    print(f"  FLOPs: {format_number(layernorm_flops)}")
    
    total_params += layernorm_params
    total_flops += layernorm_flops
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»è®¡:")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"  æ€»FLOPs: {format_number(total_flops)}")
    
    # æ¿€æ´»å‚æ•°é‡ï¼ˆå®é™…ä½¿ç”¨çš„å‚æ•°ï¼‰
    activated_params = (embedding_params + 
                       total_attn_params + 
                       (activated_up_params + activated_down_params + router_params) * num_layers +
                       layernorm_params)
    
    print(f"  æ¿€æ´»å‚æ•°é‡: {format_number(activated_params)}")
    print(f"  å‚æ•°åˆ©ç”¨ç‡: {activated_params/total_params*100:.1f}%")
    
    # è¯¦ç»†å‚æ•°åˆ†è§£
    print(f"\nğŸ“ˆ å‚æ•°åˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_params)} ({embedding_params/total_params*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_params)} ({total_attn_params/total_params*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_params)} ({total_moe_params/total_params*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_params)} ({layernorm_params/total_params*100:.1f}%)")
    
    # FLOPsåˆ†è§£
    print(f"\nâš¡ FLOPsåˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_flops)} ({embedding_flops/total_flops*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_flops)} ({total_attn_flops/total_flops*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_flops)} ({total_moe_flops/total_flops*100:.1f}%)")
    print(f"  Output: {format_number(output_embedding_flops)} ({output_embedding_flops/total_flops*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_flops)} ({layernorm_flops/total_flops*100:.1f}%)")
    
    # å†…å­˜ä¼°ç®—
    if show_memory:
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨é‡ä¼°ç®—:")
        print("-" * 40)
        
        # æ¨¡å‹æƒé‡å†…å­˜ (fp16)
        model_memory = activated_params * 2  # fp16 = 2 bytes
        
        # æ¿€æ´»å†…å­˜ä¼°ç®— (ç²—ç•¥ä¼°è®¡)
        # æ¿€æ´»å†…å­˜ä¸»è¦åŒ…æ‹¬ï¼šattentionä¸­é—´ç»“æœã€MoEä¸­é—´ç»“æœ
        attention_activation = batch_size * seq_len * hidden_size * 8  # å¤šä¸ªä¸­é—´tensor
        moe_activation = batch_size * seq_len * intermediate_size * 2 * num_activated_experts  # MoEä¸­é—´ç»“æœï¼Œæ¿€æ´»8ä¸ªä¸“å®¶
        activation_memory = (attention_activation + moe_activation) * num_layers * 2  # fp16
        
        # KV Cache (å‡è®¾å…¨éƒ¨ç¼“å­˜)
        kv_cache_memory = batch_size * seq_len * num_kv_heads * head_dim * 2 * num_layers * 2  # k+v, fp16
        
        total_memory = model_memory + activation_memory + kv_cache_memory
        
        print(f"  æ¨¡å‹æƒé‡å†…å­˜: {format_number(model_memory)}B")
        print(f"  æ¿€æ´»å†…å­˜: {format_number(activation_memory)}B")
        print(f"  KV Cacheå†…å­˜: {format_number(kv_cache_memory)}B")
        print(f"  æ€»å†…å­˜ä¼°ç®—: {format_number(total_memory)}B")
        print(f"  çº¦ {total_memory/(1024**3):.1f} GB")
    
    return {
        'total_params': total_params,
        'activated_params': activated_params,
        'total_flops': total_flops,
        'params_breakdown': {
            'embedding': embedding_params,
            'attention': total_attn_params,
            'moe': total_moe_params,
            'layernorm': layernorm_params
        },
        'flops_breakdown': {
            'embedding': embedding_flops,
            'attention': total_attn_flops,
            'moe': total_moe_flops,
            'output': output_embedding_flops,
            'layernorm': layernorm_flops
        }
    }

def calculate_model_decode_stats(batch_size=1, prefix_length=2048, show_memory=True):
    """
    è®¡ç®—æ¨¡å‹è§£ç é˜¶æ®µçš„FLOPså’Œå‚æ•°é‡
    åœ¨è§£ç é˜¶æ®µï¼Œæ¯æ¬¡åªç”Ÿæˆ1ä¸ªæ–°tokenï¼Œä½†éœ€è¦ä¸ä¹‹å‰çš„æ‰€æœ‰tokensè¿›è¡Œattentionè®¡ç®—
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        prefix_length (int): å‰ç¼€é•¿åº¦ï¼ˆåŒ…æ‹¬promptå’Œä¹‹å‰ç”Ÿæˆçš„tokensï¼‰
        show_memory (bool): æ˜¯å¦æ˜¾ç¤ºå†…å­˜ä¼°ç®—
    """
    
    # æ¨¡å‹åŸºæœ¬å‚æ•°
    vocab_size = 151936
    hidden_size = 4096
    num_layers = 94
    
    # Attentionå‚æ•°
    num_q_heads = 64
    num_kv_heads = 4  # GQA (Grouped Query Attention)
    head_dim = 128
    
    # MoEå‚æ•°
    num_experts = 128
    num_activated_experts = 8
    intermediate_size = 1536  # up projectionçš„ä¸€åŠ
    
    # è§£ç é˜¶æ®µï¼šæ¯æ¬¡å¤„ç†1ä¸ªæ–°token
    new_token_len = 1
    
    print(f"æ¨¡å‹é…ç½® (è§£ç é˜¶æ®µ):")
    print(f"  è¯æ±‡è¡¨å¤§å°: {format_number(vocab_size)}")
    print(f"  éšè—ç»´åº¦: {format_number(hidden_size)}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  Qå¤´æ•°: {num_q_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"  ä¸“å®¶æ€»æ•°: {num_experts}, æ¿€æ´»ä¸“å®¶æ•°: {num_activated_experts}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  å‰ç¼€é•¿åº¦: {format_number(prefix_length)} (åŒ…æ‹¬promptå’Œå·²ç”Ÿæˆtokens)")
    print(f"  æ–°tokené•¿åº¦: {new_token_len}")
    print("\n" + "="*80)
    
    total_params = 0
    total_flops = 0
    
    # 1. Input Embedding (åªå¤„ç†æ–°çš„1ä¸ªtoken)
    print("\n1. Input Embedding Layer")
    embedding_params = vocab_size * hidden_size
    embedding_flops = batch_size * new_token_len * hidden_size  # lookupæ“ä½œ
    
    print(f"  å‚æ•°é‡: {format_number(embedding_params)}")
    print(f"  FLOPs: {format_number(embedding_flops)}")
    
    total_params += embedding_params
    total_flops += embedding_flops
    
    # 2. Attention Layers (x94)
    print(f"\n2. Attention Layers (x{num_layers})")
    
    # Q projection (åªå¯¹æ–°tokenè®¡ç®—)
    q_proj_params = hidden_size * (num_q_heads * head_dim)
    q_proj_flops = 2 * batch_size * new_token_len * hidden_size * (num_q_heads * head_dim)
    
    # K, V projection (åªå¯¹æ–°tokenè®¡ç®—)
    kv_proj_params = hidden_size * (num_kv_heads * head_dim * 2)
    kv_proj_flops = 2 * batch_size * new_token_len * hidden_size * (num_kv_heads * head_dim * 2)
    
    # Attention computation
    # Q * K^T: 1ä¸ªæ–°queryä¸prefix_lengthä¸ªkeysåšè®¡ç®—
    qk_flops = 2 * batch_size * num_q_heads * new_token_len * prefix_length * head_dim
    # Softmax: å¯¹prefix_lengthé•¿åº¦åšsoftmax
    softmax_flops = batch_size * num_q_heads * new_token_len * prefix_length * 3
    # Attention weights * V: attention weightsä¸prefix_lengthä¸ªvaluesè®¡ç®—
    av_flops = 2 * batch_size * num_q_heads * new_token_len * prefix_length * head_dim
    
    # O projection (åªå¯¹æ–°tokençš„è¾“å‡ºåšprojection)
    o_proj_params = (num_q_heads * head_dim) * hidden_size
    o_proj_flops = 2 * batch_size * new_token_len * (num_q_heads * head_dim) * hidden_size
    
    # æ¯å±‚attentionçš„æ€»è®¡
    attn_params_per_layer = q_proj_params + kv_proj_params + o_proj_params
    attn_flops_per_layer = (q_proj_flops + kv_proj_flops + 
                           qk_flops + softmax_flops + av_flops + o_proj_flops)
    
    total_attn_params = attn_params_per_layer * num_layers
    total_attn_flops = attn_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(attn_params_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_params)}")
    print(f"    - KV projection: {format_number(kv_proj_params)}")
    print(f"    - O projection: {format_number(o_proj_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(attn_flops_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_flops)}")
    print(f"    - KV projection: {format_number(kv_proj_flops)}")
    print(f"    - QK computation: {format_number(qk_flops)}")
    print(f"    - Softmax: {format_number(softmax_flops)}")
    print(f"    - AttentionÃ—V: {format_number(av_flops)}")
    print(f"    - O projection: {format_number(o_proj_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_attn_params)}")
    print(f"  æ€»FLOPs: {format_number(total_attn_flops)}")
    
    total_params += total_attn_params
    total_flops += total_attn_flops
    
    # 3. MoE Layers (x94) - åªå¤„ç†æ–°token
    print(f"\n3. MoE Layers (x{num_layers})")
    
    # Up projection (gate + up) - åªå¯¹æ–°tokenè®¡ç®—
    up_params_per_expert = hidden_size * (intermediate_size * 2)
    total_up_params = up_params_per_expert * num_experts
    activated_up_params = up_params_per_expert * num_activated_experts
    up_flops = 2 * batch_size * new_token_len * hidden_size * (intermediate_size * 2) * num_activated_experts
    
    # Down projection - åªå¯¹æ–°tokenè®¡ç®—
    down_params_per_expert = intermediate_size * hidden_size
    total_down_params = down_params_per_expert * num_experts
    activated_down_params = down_params_per_expert * num_activated_experts
    down_flops = 2 * batch_size * new_token_len * intermediate_size * hidden_size * num_activated_experts
    
    # æ¿€æ´»å‡½æ•° (SiLU + ä¹˜æ³•) - åªå¯¹æ–°tokenè®¡ç®—
    activation_flops = batch_size * new_token_len * intermediate_size * num_activated_experts * 3
    
    # è·¯ç”±ç½‘ç»œ - åªå¯¹æ–°tokenè®¡ç®—
    router_params = hidden_size * num_experts
    router_flops = 2 * batch_size * new_token_len * hidden_size * num_experts
    
    # æ¯å±‚MoEçš„æ€»è®¡
    moe_params_per_layer = total_up_params + total_down_params + router_params
    moe_flops_per_layer = up_flops + down_flops + activation_flops + router_flops
    
    total_moe_params = moe_params_per_layer * num_layers
    total_moe_flops = moe_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(moe_params_per_layer)}")
    print(f"    - Up projection (æ€»): {format_number(total_up_params)}")
    print(f"    - Down projection (æ€»): {format_number(total_down_params)}")
    print(f"    - Router: {format_number(router_params)}")
    print(f"  æ¯å±‚æ¿€æ´»å‚æ•°é‡: {format_number(activated_up_params + activated_down_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(moe_flops_per_layer)}")
    print(f"    - Up projection: {format_number(up_flops)}")
    print(f"    - Down projection: {format_number(down_flops)}")
    print(f"    - Activation: {format_number(activation_flops)}")
    print(f"    - Router: {format_number(router_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_moe_params)}")
    print(f"  æ€»FLOPs: {format_number(total_moe_flops)}")
    
    total_params += total_moe_params
    total_flops += total_moe_flops
    
    # 4. Output Embedding (åªå¯¹æ–°tokenè®¡ç®—)
    print(f"\n4. Output Embedding Layer")
    output_embedding_params = 0  # é€šå¸¸å…±äº«æƒé‡
    output_embedding_flops = 2 * batch_size * new_token_len * hidden_size * vocab_size
    
    print(f"  å‚æ•°é‡: {format_number(output_embedding_params)} (å…±äº«æƒé‡)")
    print(f"  FLOPs: {format_number(output_embedding_flops)}")
    
    total_flops += output_embedding_flops
    
    # 5. Layer Norm (åªå¯¹æ–°tokenè®¡ç®—)
    print(f"\n5. Layer Normalization")
    # æ¯å±‚æœ‰2ä¸ªLayerNorm: attentionåå’ŒMoEå
    layernorm_params = hidden_size * 2 * num_layers * 2  # scaleå’Œbias
    layernorm_flops = batch_size * new_token_len * hidden_size * 5 * 2 * num_layers
    
    print(f"  å‚æ•°é‡: {format_number(layernorm_params)}")
    print(f"  FLOPs: {format_number(layernorm_flops)}")
    
    total_params += layernorm_params
    total_flops += layernorm_flops
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»è®¡ (è§£ç é˜¶æ®µ - ç”Ÿæˆ1ä¸ªtoken):")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"  æ€»FLOPs: {format_number(total_flops)}")
    
    # æ¿€æ´»å‚æ•°é‡ï¼ˆå®é™…ä½¿ç”¨çš„å‚æ•°ï¼‰
    activated_params = (embedding_params + 
                       total_attn_params + 
                       (activated_up_params + activated_down_params + router_params) * num_layers +
                       layernorm_params)
    
    print(f"  æ¿€æ´»å‚æ•°é‡: {format_number(activated_params)}")
    print(f"  å‚æ•°åˆ©ç”¨ç‡: {activated_params/total_params*100:.1f}%")
    
    # è¯¦ç»†å‚æ•°åˆ†è§£
    print(f"\nğŸ“ˆ å‚æ•°åˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_params)} ({embedding_params/total_params*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_params)} ({total_attn_params/total_params*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_params)} ({total_moe_params/total_params*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_params)} ({layernorm_params/total_params*100:.1f}%)")
    
    # FLOPsåˆ†è§£
    print(f"\nâš¡ FLOPsåˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_flops)} ({embedding_flops/total_flops*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_flops)} ({total_attn_flops/total_flops*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_flops)} ({total_moe_flops/total_flops*100:.1f}%)")
    print(f"  Output: {format_number(output_embedding_flops)} ({output_embedding_flops/total_flops*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_flops)} ({layernorm_flops/total_flops*100:.1f}%)")
    
    # å†…å­˜ä¼°ç®—
    if show_memory:
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨é‡ä¼°ç®— (è§£ç é˜¶æ®µ):")
        print("-" * 40)
        
        # æ¨¡å‹æƒé‡å†…å­˜ (fp16) - ä¸é¢„å¡«å……é˜¶æ®µç›¸åŒ
        model_memory = activated_params * 2  # fp16 = 2 bytes
        
        # æ¿€æ´»å†…å­˜ä¼°ç®— (è§£ç é˜¶æ®µåªå¤„ç†1ä¸ªtokenï¼Œå†…å­˜éœ€æ±‚å¤§å¤§é™ä½)
        attention_activation = batch_size * new_token_len * hidden_size * 8  # å•ä¸ªtokençš„attentionä¸­é—´ç»“æœ
        moe_activation = batch_size * new_token_len * intermediate_size * 2 * num_activated_experts  # å•ä¸ªtokençš„MoEä¸­é—´ç»“æœ
        activation_memory = (attention_activation + moe_activation) * num_layers * 2  # fp16
        
        # KV Cache (å­˜å‚¨åˆ°prefix_lengthçš„æ‰€æœ‰å†å²)
        kv_cache_memory = batch_size * prefix_length * num_kv_heads * head_dim * 2 * num_layers * 2  # k+v, fp16
        
        total_memory = model_memory + activation_memory + kv_cache_memory
        
        print(f"  æ¨¡å‹æƒé‡å†…å­˜: {format_number(model_memory)}B")
        print(f"  æ¿€æ´»å†…å­˜: {format_number(activation_memory)}B")
        print(f"  KV Cacheå†…å­˜: {format_number(kv_cache_memory)}B")
        print(f"  æ€»å†…å­˜ä¼°ç®—: {format_number(total_memory)}B")
        print(f"  çº¦ {total_memory/(1024**3):.1f} GB")
        
        # ä¸é¢„å¡«å……é˜¶æ®µçš„å¯¹æ¯”
        print(f"\nğŸ”„ è§£ç  vs é¢„å¡«å……å¯¹æ¯”:")
        print(f"  æ¿€æ´»å†…å­˜å‡å°‘: ~{new_token_len/prefix_length*100:.1f}% (å•token vs {format_number(prefix_length)}tokens)")
        print(f"  KV Cache: ç›¸åŒå¤§å° (éœ€è¦ä¿å­˜å®Œæ•´å†å²)")
    
    return {
        'total_params': total_params,
        'activated_params': activated_params,
        'total_flops': total_flops,
        'prefix_length': prefix_length,
        'params_breakdown': {
            'embedding': embedding_params,
            'attention': total_attn_params,
            'moe': total_moe_params,
            'layernorm': layernorm_params
        },
        'flops_breakdown': {
            'embedding': embedding_flops,
            'attention': total_attn_flops,
            'moe': total_moe_flops,
            'output': output_embedding_flops,
            'layernorm': layernorm_flops
        }
    }

def compare_sequence_lengths():
    """æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„è®¡ç®—é‡"""
    print("\n" + "="*80)
    print("ğŸ” ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„FLOPså¯¹æ¯”:")
    print("="*80)
    
    seq_lengths = [512, 1024, 2048, 4096, 8192]
    
    print(f"{'åºåˆ—é•¿åº¦':<10} {'æ€»FLOPs':<12} {'Attention FLOPs':<15} {'MoE FLOPs':<12}")
    print("-" * 55)
    
    for seq_len in seq_lengths:
        stats = calculate_model_prefill_stats(batch_size=1, seq_len=seq_len, show_memory=False)
        attn_flops = stats['flops_breakdown']['attention']
        moe_flops = stats['flops_breakdown']['moe']
        total_flops = stats['total_flops']
        
        print(f"{seq_len:<10} {format_number(total_flops):<12} {format_number(attn_flops):<15} {format_number(moe_flops):<12}")

def compare_prefill_vs_decode():
    """æ¯”è¾ƒé¢„å¡«å……é˜¶æ®µå’Œè§£ç é˜¶æ®µçš„è®¡ç®—é‡"""
    print("\n" + "="*80)
    print("ğŸ”„ é¢„å¡«å…… vs è§£ç é˜¶æ®µå¯¹æ¯”:")
    print("="*80)
    
    batch_size = 1
    seq_len = 2048
    prefix_length = 2048
    
    print(f"å¯¹æ¯”é…ç½®: batch_size={batch_size}, seq_len={seq_len}, prefix_length={prefix_length}")
    print("-" * 80)
    
    # é¢„å¡«å……é˜¶æ®µç»Ÿè®¡
    prefill_stats = calculate_model_prefill_stats(batch_size=batch_size, seq_len=seq_len, show_memory=False)
    
    # è§£ç é˜¶æ®µç»Ÿè®¡  
    decode_stats = calculate_model_decode_stats(batch_size=batch_size, prefix_length=prefix_length, show_memory=False)
    
    print(f"\nğŸ“Š FLOPså¯¹æ¯”:")
    print(f"{'é˜¶æ®µ':<12} {'æ€»FLOPs':<15} {'Attention':<15} {'MoE':<15} {'Output':<15}")
    print("-" * 75)
    
    prefill_total = prefill_stats['total_flops']
    prefill_attn = prefill_stats['flops_breakdown']['attention']  
    prefill_moe = prefill_stats['flops_breakdown']['moe']
    prefill_output = prefill_stats['flops_breakdown']['output']
    
    decode_total = decode_stats['total_flops']
    decode_attn = decode_stats['flops_breakdown']['attention']
    decode_moe = decode_stats['flops_breakdown']['moe'] 
    decode_output = decode_stats['flops_breakdown']['output']
    
    print(f"{'é¢„å¡«å……':<12} {format_number(prefill_total):<15} {format_number(prefill_attn):<15} {format_number(prefill_moe):<15} {format_number(prefill_output):<15}")
    print(f"{'è§£ç ':<12} {format_number(decode_total):<15} {format_number(decode_attn):<15} {format_number(decode_moe):<15} {format_number(decode_output):<15}")
    print(f"{'æ¯”ç‡':<12} {decode_total/prefill_total:.3f}{'x':<14} {decode_attn/prefill_attn:.3f}{'x':<14} {decode_moe/prefill_moe:.3f}{'x':<14} {decode_output/prefill_output:.3f}{'x':<14}")
    
    print(f"\nğŸ’¡ å…³é”®è§‚å¯Ÿ:")
    print(f"  - è§£ç é˜¶æ®µæ€»FLOPsæ˜¯é¢„å¡«å……é˜¶æ®µçš„ {decode_total/prefill_total:.2f}x")
    print(f"  - Attentionè®¡ç®—åœ¨è§£ç é˜¶æ®µæ˜¾è‘—é™ä½ ({decode_attn/prefill_attn:.3f}x)")
    print(f"  - MoEè®¡ç®—å¤§å¹…é™ä½ ({decode_moe/prefill_moe:.3f}x)")
    print(f"  - æ¯ä¸ªtokençš„è§£ç æˆæœ¬è¿œä½äºé¢„å¡«å……é˜¶æ®µ")

if __name__ == "__main__":
    print("ğŸš€ Qwen3-235B æ¨¡å‹åˆ†æ")
    print("="*80)
    
    # è®¡ç®—é¢„å¡«å……é˜¶æ®µç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ”¥ é¢„å¡«å……é˜¶æ®µåˆ†æ:")
    calculate_model_prefill_stats(batch_size=2, seq_len=4096, show_memory=True)
    
    # è®¡ç®—è§£ç é˜¶æ®µç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ¯ è§£ç é˜¶æ®µåˆ†æ:")
    calculate_model_decode_stats(batch_size=128, prefix_length=4096, show_memory=True)
    
    # é¢„å¡«å……vsè§£ç å¯¹æ¯”
    compare_prefill_vs_decode()
    
    # æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    # compare_sequence_lengths() 