#!/usr/bin/env python3
"""
æ¨¡å‹FLOPså’Œå‚æ•°é‡è®¡ç®—è„šæœ¬
æ ¹æ®æä¾›çš„æ¨¡å‹æ¶æ„ä¿¡æ¯è®¡ç®—æ¯ä¸ªæ¨¡å—çš„è®¡ç®—é‡å’Œå‚æ•°é‡
æ‰€æœ‰æ•°å€¼éƒ½ä»¥ç›´è§‚çš„å•ä½æ˜¾ç¤º

FLOPsè®¡ç®—è¯´æ˜ï¼š
- å¯¹äºçŸ©é˜µä¹˜æ³• A(mÃ—k) Ã— B(kÃ—n)ï¼ŒåŒ…å« mÃ—nÃ—k æ¬¡ä¹˜æ³•å’Œ mÃ—nÃ—(k-1) æ¬¡åŠ æ³•
- æ€»FLOPs = mÃ—nÃ—(2k-1) â‰ˆ 2Ã—mÃ—nÃ—k
- çº¿æ€§å±‚çš„FLOPséƒ½ä¹˜ä»¥2æ¥åŒ…å«ä¹˜æ³•å’ŒåŠ æ³•è¿ç®—
- éçŸ©é˜µä¹˜æ³•è¿ç®—ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ã€softmaxï¼‰ä¸ä¹˜2
"""

def format_number(num):
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸ºæ›´ç›´è§‚çš„å•ä½"""
    if num >= 1e12:
        return f"{num/1e12:.2f}T"
    elif num >= 1e9:
        return f"{num/1e9:.2f}B"
    elif num >= 1e6:
        return f"{num/1e6:.2f}M"
    elif num >= 1e3:
        return f"{num/1e3:.2f}K"
    else:
        return f"{num:.0f}"

def calculate_model_stats(batch_size=1, seq_len=2048, show_memory=True):
    """
    è®¡ç®—æ¨¡å‹çš„FLOPså’Œå‚æ•°é‡
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        seq_len (int): åºåˆ—é•¿åº¦
        show_memory (bool): æ˜¯å¦æ˜¾ç¤ºå†…å­˜ä¼°ç®—
    """
    
    # æ¨¡å‹åŸºæœ¬å‚æ•°
    vocab_size = 151936
    hidden_size = 4096
    num_layers = 94
    
    # Attentionå‚æ•°
    num_q_heads = 64
    num_kv_heads = 4  # GQA (Grouped Query Attention)
    head_dim = 128
    
    # MoEå‚æ•°
    num_experts = 128
    num_activated_experts = 8
    intermediate_size = 1536  # up projectionçš„ä¸€åŠ
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {format_number(vocab_size)}")
    print(f"  éšè—ç»´åº¦: {format_number(hidden_size)}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  Qå¤´æ•°: {num_q_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"  ä¸“å®¶æ€»æ•°: {num_experts}, æ¿€æ´»ä¸“å®¶æ•°: {num_activated_experts}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {format_number(seq_len)}")
    print("\n" + "="*80)
    
    total_params = 0
    total_flops = 0
    
    # 1. Input Embedding
    print("\n1. Input Embedding Layer")
    embedding_params = vocab_size * hidden_size
    embedding_flops = batch_size * seq_len * hidden_size  # lookupæ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
    
    print(f"  å‚æ•°é‡: {format_number(embedding_params)}")
    print(f"  FLOPs: {format_number(embedding_flops)}")
    
    total_params += embedding_params
    total_flops += embedding_flops
    
    # 2. Attention Layers (x94)
    print(f"\n2. Attention Layers (x{num_layers})")
    
    # Q projection
    q_proj_params = hidden_size * (num_q_heads * head_dim)
    q_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_q_heads * head_dim)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # K, V projection  
    kv_proj_params = hidden_size * (num_kv_heads * head_dim * 2)
    kv_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_kv_heads * head_dim * 2)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Attention computation
    # Q * K^T
    qk_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    # Softmax (è¿‘ä¼¼)
    softmax_flops = batch_size * num_q_heads * seq_len * seq_len * 3  # softmaxä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    # Attention weights * V
    av_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # O projection
    o_proj_params = (num_q_heads * head_dim) * hidden_size
    o_proj_flops = 2 * batch_size * seq_len * (num_q_heads * head_dim) * hidden_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚attentionçš„æ€»è®¡
    attn_params_per_layer = q_proj_params + kv_proj_params + o_proj_params
    attn_flops_per_layer = (q_proj_flops + kv_proj_flops + 
                           qk_flops + softmax_flops + av_flops + o_proj_flops)
    
    total_attn_params = attn_params_per_layer * num_layers
    total_attn_flops = attn_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(attn_params_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_params)}")
    print(f"    - KV projection: {format_number(kv_proj_params)}")
    print(f"    - O projection: {format_number(o_proj_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(attn_flops_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_flops)}")
    print(f"    - KV projection: {format_number(kv_proj_flops)}")
    print(f"    - QK computation: {format_number(qk_flops)}")
    print(f"    - Softmax: {format_number(softmax_flops)}")
    print(f"    - AttentionÃ—V: {format_number(av_flops)}")
    print(f"    - O projection: {format_number(o_proj_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_attn_params)}")
    print(f"  æ€»FLOPs: {format_number(total_attn_flops)}")
    
    total_params += total_attn_params
    total_flops += total_attn_flops
    
    # 3. MoE Layers (x94)
    print(f"\n3. MoE Layers (x{num_layers})")
    
    # Up projection (gate + up)
    up_params_per_expert = hidden_size * (intermediate_size * 2)  # gateå’Œupåˆå¹¶
    total_up_params = up_params_per_expert * num_experts
    activated_up_params = up_params_per_expert * num_activated_experts
    up_flops = 2 * batch_size * seq_len * hidden_size * (intermediate_size * 2) * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Down projection
    down_params_per_expert = intermediate_size * hidden_size
    total_down_params = down_params_per_expert * num_experts
    activated_down_params = down_params_per_expert * num_activated_experts
    down_flops = 2 * batch_size * seq_len * intermediate_size * hidden_size * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¿€æ´»å‡½æ•° (SiLU + ä¹˜æ³•)
    activation_flops = batch_size * seq_len * intermediate_size * num_activated_experts * 3  # æ¿€æ´»å‡½æ•°ä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    
    # è·¯ç”±ç½‘ç»œ
    router_params = hidden_size * num_experts
    router_flops = 2 * batch_size * seq_len * hidden_size * num_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚MoEçš„æ€»è®¡
    moe_params_per_layer = total_up_params + total_down_params + router_params
    moe_flops_per_layer = up_flops + down_flops + activation_flops + router_flops
    
    total_moe_params = moe_params_per_layer * num_layers
    total_moe_flops = moe_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(moe_params_per_layer)}")
    print(f"    - Up projection (æ€»): {format_number(total_up_params)}")
    print(f"    - Down projection (æ€»): {format_number(total_down_params)}")
    print(f"    - Router: {format_number(router_params)}")
    print(f"  æ¯å±‚æ¿€æ´»å‚æ•°é‡: {format_number(activated_up_params + activated_down_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(moe_flops_per_layer)}")
    print(f"    - Up projection: {format_number(up_flops)}")
    print(f"    - Down projection: {format_number(down_flops)}")
    print(f"    - Activation: {format_number(activation_flops)}")
    print(f"    - Router: {format_number(router_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_moe_params)}")
    print(f"  æ€»FLOPs: {format_number(total_moe_flops)}")
    
    total_params += total_moe_params
    total_flops += total_moe_flops
    
    # 4. Output Embedding (é€šå¸¸ä¸input embeddingå…±äº«æƒé‡)
    print(f"\n4. Output Embedding Layer")
    output_embedding_params = 0  # é€šå¸¸å…±äº«æƒé‡
    output_embedding_flops = 2 * batch_size * seq_len * hidden_size * vocab_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    print(f"  å‚æ•°é‡: {format_number(output_embedding_params)} (å…±äº«æƒé‡)")
    print(f"  FLOPs: {format_number(output_embedding_flops)}")
    
    total_flops += output_embedding_flops
    
    # 5. Layer Norm (ä¼°ç®—)
    print(f"\n5. Layer Normalization")
    # æ¯å±‚æœ‰2ä¸ªLayerNorm: attentionåå’ŒMoEå
    layernorm_params = hidden_size * 2 * num_layers * 2  # scaleå’Œbias
    layernorm_flops = batch_size * seq_len * hidden_size * 5 * 2 * num_layers  # æ¯ä¸ªLNçº¦5ä¸ªæ“ä½œ
    
    print(f"  å‚æ•°é‡: {format_number(layernorm_params)}")
    print(f"  FLOPs: {format_number(layernorm_flops)}")
    
    total_params += layernorm_params
    total_flops += layernorm_flops
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»è®¡:")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"  æ€»FLOPs: {format_number(total_flops)}")
    
    # æ¿€æ´»å‚æ•°é‡ï¼ˆå®é™…ä½¿ç”¨çš„å‚æ•°ï¼‰
    activated_params = (embedding_params + 
                       total_attn_params + 
                       (activated_up_params + activated_down_params + router_params) * num_layers +
                       layernorm_params)
    
    print(f"  æ¿€æ´»å‚æ•°é‡: {format_number(activated_params)}")
    print(f"  å‚æ•°åˆ©ç”¨ç‡: {activated_params/total_params*100:.1f}%")
    
    # è¯¦ç»†å‚æ•°åˆ†è§£
    print(f"\nğŸ“ˆ å‚æ•°åˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_params)} ({embedding_params/total_params*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_params)} ({total_attn_params/total_params*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_params)} ({total_moe_params/total_params*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_params)} ({layernorm_params/total_params*100:.1f}%)")
    
    # FLOPsåˆ†è§£
    print(f"\nâš¡ FLOPsåˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_flops)} ({embedding_flops/total_flops*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_flops)} ({total_attn_flops/total_flops*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_flops)} ({total_moe_flops/total_flops*100:.1f}%)")
    print(f"  Output: {format_number(output_embedding_flops)} ({output_embedding_flops/total_flops*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_flops)} ({layernorm_flops/total_flops*100:.1f}%)")
    
    # å†…å­˜ä¼°ç®—
    if show_memory:
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨é‡ä¼°ç®—:")
        print("-" * 40)
        
        # æ¨¡å‹æƒé‡å†…å­˜ (fp16)
        model_memory = activated_params * 2  # fp16 = 2 bytes
        
        # æ¿€æ´»å†…å­˜ä¼°ç®— (ç²—ç•¥ä¼°è®¡)
        # æ¿€æ´»å†…å­˜ä¸»è¦åŒ…æ‹¬ï¼šattentionä¸­é—´ç»“æœã€MoEä¸­é—´ç»“æœ
        attention_activation = batch_size * seq_len * hidden_size * 8  # å¤šä¸ªä¸­é—´tensor
        moe_activation = batch_size * seq_len * intermediate_size * 2 * num_activated_experts  # MoEä¸­é—´ç»“æœï¼Œæ¿€æ´»8ä¸ªä¸“å®¶
        activation_memory = (attention_activation + moe_activation) * num_layers * 2  # fp16
        
        # KV Cache (å‡è®¾å…¨éƒ¨ç¼“å­˜)
        kv_cache_memory = batch_size * seq_len * num_kv_heads * head_dim * 2 * num_layers * 2  # k+v, fp16
        
        total_memory = model_memory + activation_memory + kv_cache_memory
        
        print(f"  æ¨¡å‹æƒé‡å†…å­˜: {format_number(model_memory)}B")
        print(f"  æ¿€æ´»å†…å­˜: {format_number(activation_memory)}B")
        print(f"  KV Cacheå†…å­˜: {format_number(kv_cache_memory)}B")
        print(f"  æ€»å†…å­˜ä¼°ç®—: {format_number(total_memory)}B")
        print(f"  çº¦ {total_memory/(1024**3):.1f} GB")
    
    return {
        'total_params': total_params,
        'activated_params': activated_params,
        'total_flops': total_flops,
        'params_breakdown': {
            'embedding': embedding_params,
            'attention': total_attn_params,
            'moe': total_moe_params,
            'layernorm': layernorm_params
        },
        'flops_breakdown': {
            'embedding': embedding_flops,
            'attention': total_attn_flops,
            'moe': total_moe_flops,
            'output': output_embedding_flops,
            'layernorm': layernorm_flops
        }
    }

def compare_sequence_lengths():
    """æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„è®¡ç®—é‡"""
    print("\n" + "="*80)
    print("ğŸ” ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„FLOPså¯¹æ¯”:")
    print("="*80)
    
    seq_lengths = [512, 1024, 2048, 4096, 8192]
    
    print(f"{'åºåˆ—é•¿åº¦':<10} {'æ€»FLOPs':<12} {'Attention FLOPs':<15} {'MoE FLOPs':<12}")
    print("-" * 55)
    
    for seq_len in seq_lengths:
        stats = calculate_model_stats(batch_size=1, seq_len=seq_len, show_memory=False)
        attn_flops = stats['flops_breakdown']['attention']
        moe_flops = stats['flops_breakdown']['moe']
        total_flops = stats['total_flops']
        
        print(f"{seq_len:<10} {format_number(total_flops):<12} {format_number(attn_flops):<15} {format_number(moe_flops):<12}")

if __name__ == "__main__":
    print("ğŸš€ Qwen3-235B æ¨¡å‹åˆ†æ")
    print("="*80)
    
    # è®¡ç®—é»˜è®¤é…ç½®ä¸‹çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«å†…å­˜ä¼°ç®—ï¼‰
    calculate_model_stats(batch_size=4, seq_len=2048, show_memory=True)
    
    # æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ï¼ˆä¸æ˜¾ç¤ºå†…å­˜ï¼Œé¿å…é‡å¤ï¼‰
    # compare_sequence_lengths() 