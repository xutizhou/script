#!/usr/bin/env python3
"""
æ¨¡å‹FLOPså’Œå‚æ•°é‡è®¡ç®—è„šæœ¬
æ ¹æ®æä¾›çš„æ¨¡å‹æ¶æ„ä¿¡æ¯è®¡ç®—æ¯ä¸ªæ¨¡å—çš„è®¡ç®—é‡å’Œå‚æ•°é‡
æ‰€æœ‰æ•°å€¼éƒ½ä»¥ç›´è§‚çš„å•ä½æ˜¾ç¤º

FLOPsè®¡ç®—è¯´æ˜ï¼š
- å¯¹äºçŸ©é˜µä¹˜æ³• A(mÃ—k) Ã— B(kÃ—n)ï¼ŒåŒ…å« mÃ—nÃ—k æ¬¡ä¹˜æ³•å’Œ mÃ—nÃ—(k-1) æ¬¡åŠ æ³•
- æ€»FLOPs = mÃ—nÃ—(2k-1) â‰ˆ 2Ã—mÃ—nÃ—k
- çº¿æ€§å±‚çš„FLOPséƒ½ä¹˜ä»¥2æ¥åŒ…å«ä¹˜æ³•å’ŒåŠ æ³•è¿ç®—
- éçŸ©é˜µä¹˜æ³•è¿ç®—ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ã€softmaxï¼‰ä¸ä¹˜2
"""

def format_number(num):
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸ºæ›´ç›´è§‚çš„å•ä½"""
    if num >= 1e12:
        return f"{num/1e12:.2f}T"
    elif num >= 1e9:
        return f"{num/1e9:.2f}B"
    elif num >= 1e6:
        return f"{num/1e6:.2f}M"
    elif num >= 1e3:
        return f"{num/1e3:.2f}K"
    else:
        return f"{num:.0f}"

def estimate_time_us(flops, mem_bytes, tflops, gpu_mem_bandwidth_gbps):
    """æ ¹æ®FLOPså’Œè®¿å­˜é‡ä¼°ç®—ç†è®ºæ—¶é—´ï¼ˆusï¼‰"""
    # Time based on compute (FLOPs)
    compute_time_s = (flops / (tflops * 1e12)) if tflops > 0 else 0
    
    # Time based on memory access (Bytes)
    mem_time_s = (mem_bytes / (gpu_mem_bandwidth_gbps * 1e9)) if gpu_mem_bandwidth_gbps > 0 else 0
    
    # Convert to microseconds
    compute_time_us = compute_time_s * 1e6
    mem_time_us = mem_time_s * 1e6
    
    return compute_time_us, mem_time_us

def calculate_model_prefill_stats(batch_size=1, seq_len=2048, show_memory=True):
    """
    è®¡ç®—æ¨¡å‹çš„FLOPså’Œå‚æ•°é‡
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        seq_len (int): åºåˆ—é•¿åº¦
        show_memory (bool): æ˜¯å¦æ˜¾ç¤ºå†…å­˜ä¼°ç®—
    """
    
    # æ¨¡å‹åŸºæœ¬å‚æ•°
    vocab_size = 151936
    hidden_size = 4096
    num_layers = 94
    
    # Attentionå‚æ•°
    num_q_heads = 64
    num_kv_heads = 4  # GQA (Grouped Query Attention)
    head_dim = 128
    
    # MoEå‚æ•°
    num_experts = 128
    num_activated_experts = 8
    intermediate_size = 1536  # up projectionçš„ä¸€åŠ
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {format_number(vocab_size)}")
    print(f"  éšè—ç»´åº¦: {format_number(hidden_size)}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  Qå¤´æ•°: {num_q_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"  ä¸“å®¶æ€»æ•°: {num_experts}, æ¿€æ´»ä¸“å®¶æ•°: {num_activated_experts}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {format_number(seq_len)}")
    print("\n" + "="*80)
    
    total_params = 0
    total_flops = 0
    
    # 1. Input Embedding
    print("\n1. Input Embedding Layer")
    embedding_params = vocab_size * hidden_size
    embedding_flops = batch_size * seq_len * hidden_size  # lookupæ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
    
    print(f"  å‚æ•°é‡: {format_number(embedding_params)}")
    print(f"  FLOPs: {format_number(embedding_flops)}")
    
    total_params += embedding_params
    total_flops += embedding_flops
    
    # 2. Attention Layers (x94)
    print(f"\n2. Attention Layers (x{num_layers})")
    
    # Q projection
    q_proj_params = hidden_size * (num_q_heads * head_dim)
    q_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_q_heads * head_dim)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # K, V projection  
    kv_proj_params = hidden_size * (num_kv_heads * head_dim * 2)
    kv_proj_flops = 2 * batch_size * seq_len * hidden_size * (num_kv_heads * head_dim * 2)  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Attention computation
    # Q * K^T
    qk_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    # Softmax (è¿‘ä¼¼)
    softmax_flops = batch_size * num_q_heads * seq_len * seq_len * 3  # softmaxä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    # Attention weights * V
    av_flops = 2 * batch_size * num_q_heads * seq_len * seq_len * head_dim  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # O projection
    o_proj_params = (num_q_heads * head_dim) * hidden_size
    o_proj_flops = 2 * batch_size * seq_len * (num_q_heads * head_dim) * hidden_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚attentionçš„æ€»è®¡
    attn_params_per_layer = q_proj_params + kv_proj_params + o_proj_params
    attn_flops_per_layer = (q_proj_flops + kv_proj_flops + 
                           qk_flops + softmax_flops + av_flops + o_proj_flops)
    
    total_attn_params = attn_params_per_layer * num_layers
    total_attn_flops = attn_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(attn_params_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_params)}")
    print(f"    - KV projection: {format_number(kv_proj_params)}")
    print(f"    - O projection: {format_number(o_proj_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(attn_flops_per_layer)}")
    print(f"    - Q projection: {format_number(q_proj_flops)}")
    print(f"    - KV projection: {format_number(kv_proj_flops)}")
    print(f"    - QK computation: {format_number(qk_flops)}")
    print(f"    - Softmax: {format_number(softmax_flops)}")
    print(f"    - AttentionÃ—V: {format_number(av_flops)}")
    print(f"    - O projection: {format_number(o_proj_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_attn_params)}")
    print(f"  æ€»FLOPs: {format_number(total_attn_flops)}")
    
    total_params += total_attn_params
    total_flops += total_attn_flops
    
    # 3. MoE Layers (x94)
    print(f"\n3. MoE Layers (x{num_layers})")
    
    # Up projection (gate + up)
    up_params_per_expert = hidden_size * (intermediate_size * 2)  # gateå’Œupåˆå¹¶
    total_up_params = up_params_per_expert * num_experts
    activated_up_params = up_params_per_expert * num_activated_experts
    up_flops = 2 * batch_size * seq_len * hidden_size * (intermediate_size * 2) * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # Down projection
    down_params_per_expert = intermediate_size * hidden_size
    total_down_params = down_params_per_expert * num_experts
    activated_down_params = down_params_per_expert * num_activated_experts
    down_flops = 2 * batch_size * seq_len * intermediate_size * hidden_size * num_activated_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¿€æ´»å‡½æ•° (SiLU + ä¹˜æ³•)
    activation_flops = batch_size * seq_len * intermediate_size * num_activated_experts * 3  # æ¿€æ´»å‡½æ•°ä¸æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä¸ä¹˜2
    
    # è·¯ç”±ç½‘ç»œ
    router_params = hidden_size * num_experts
    router_flops = 2 * batch_size * seq_len * hidden_size * num_experts  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    # æ¯å±‚MoEçš„æ€»è®¡
    moe_params_per_layer = total_up_params + total_down_params + router_params
    moe_flops_per_layer = up_flops + down_flops + activation_flops + router_flops
    
    total_moe_params = moe_params_per_layer * num_layers
    total_moe_flops = moe_flops_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(moe_params_per_layer)}")
    print(f"    - Up projection (æ€»): {format_number(total_up_params)}")
    print(f"    - Down projection (æ€»): {format_number(total_down_params)}")
    print(f"    - Router: {format_number(router_params)}")
    print(f"  æ¯å±‚æ¿€æ´»å‚æ•°é‡: {format_number(activated_up_params + activated_down_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(moe_flops_per_layer)}")
    print(f"    - Up projection: {format_number(up_flops)}")
    print(f"    - Down projection: {format_number(down_flops)}")
    print(f"    - Activation: {format_number(activation_flops)}")
    print(f"    - Router: {format_number(router_flops)}")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_moe_params)}")
    print(f"  æ€»FLOPs: {format_number(total_moe_flops)}")
    
    total_params += total_moe_params
    total_flops += total_moe_flops
    
    # 4. Output Embedding (é€šå¸¸ä¸input embeddingå…±äº«æƒé‡)
    print(f"\n4. Output Embedding Layer")
    output_embedding_params = 0  # é€šå¸¸å…±äº«æƒé‡
    output_embedding_flops = 2 * batch_size * seq_len * hidden_size * vocab_size  # ä¹˜ä»¥2åŒ…å«ä¹˜åŠ è¿ç®—
    
    print(f"  å‚æ•°é‡: {format_number(output_embedding_params)} (å…±äº«æƒé‡)")
    print(f"  FLOPs: {format_number(output_embedding_flops)}")
    
    total_flops += output_embedding_flops
    
    # 5. Layer Norm (ä¼°ç®—)
    print(f"\n5. Layer Normalization")
    # æ¯å±‚æœ‰2ä¸ªLayerNorm: attentionåå’ŒMoEå
    layernorm_params = hidden_size * 2 * num_layers * 2  # scaleå’Œbias
    layernorm_flops = batch_size * seq_len * hidden_size * 5 * 2 * num_layers  # æ¯ä¸ªLNçº¦5ä¸ªæ“ä½œ
    
    print(f"  å‚æ•°é‡: {format_number(layernorm_params)}")
    print(f"  FLOPs: {format_number(layernorm_flops)}")
    
    total_params += layernorm_params
    total_flops += layernorm_flops
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»è®¡:")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"  æ€»FLOPs: {format_number(total_flops)}")
    
    # æ¿€æ´»å‚æ•°é‡ï¼ˆå®é™…ä½¿ç”¨çš„å‚æ•°ï¼‰
    activated_params = (embedding_params + 
                       total_attn_params + 
                       (activated_up_params + activated_down_params + router_params) * num_layers +
                       layernorm_params)
    
    print(f"  æ¿€æ´»å‚æ•°é‡: {format_number(activated_params)}")
    print(f"  å‚æ•°åˆ©ç”¨ç‡: {activated_params/total_params*100:.1f}%")
    
    # è¯¦ç»†å‚æ•°åˆ†è§£
    print(f"\nğŸ“ˆ å‚æ•°åˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_params)} ({embedding_params/total_params*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_params)} ({total_attn_params/total_params*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_params)} ({total_moe_params/total_params*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_params)} ({layernorm_params/total_params*100:.1f}%)")
    
    # FLOPsåˆ†è§£
    print(f"\nâš¡ FLOPsåˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_flops)} ({embedding_flops/total_flops*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_flops)} ({total_attn_flops/total_flops*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_flops)} ({total_moe_flops/total_flops*100:.1f}%)")
    print(f"  Output: {format_number(output_embedding_flops)} ({output_embedding_flops/total_flops*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_flops)} ({layernorm_flops/total_flops*100:.1f}%)")
    
    # å†…å­˜ä¼°ç®—
    if show_memory:
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨é‡ä¼°ç®—:")
        print("-" * 40)
        
        # æ¨¡å‹æƒé‡å†…å­˜ (fp16)
        model_memory = activated_params * 2  # fp16 = 2 bytes
        
        # æ¿€æ´»å†…å­˜ä¼°ç®— (ç²—ç•¥ä¼°è®¡)
        # æ¿€æ´»å†…å­˜ä¸»è¦åŒ…æ‹¬ï¼šattentionä¸­é—´ç»“æœã€MoEä¸­é—´ç»“æœ
        attention_activation = batch_size * seq_len * hidden_size * 8  # å¤šä¸ªä¸­é—´tensor
        moe_activation = batch_size * seq_len * intermediate_size * 2 * num_activated_experts  # MoEä¸­é—´ç»“æœï¼Œæ¿€æ´»8ä¸ªä¸“å®¶
        activation_memory = (attention_activation + moe_activation) * num_layers * 2  # fp16
        
        # KV Cache (å‡è®¾å…¨éƒ¨ç¼“å­˜)
        kv_cache_memory = batch_size * seq_len * num_kv_heads * head_dim * 2 * num_layers * 2  # k+v, fp16
        
        total_memory = model_memory + activation_memory + kv_cache_memory
        
        print(f"  æ¨¡å‹æƒé‡å†…å­˜: {format_number(model_memory)}B")
        print(f"  æ¿€æ´»å†…å­˜: {format_number(activation_memory)}B")
        print(f"  KV Cacheå†…å­˜: {format_number(kv_cache_memory)}B")
        print(f"  æ€»å†…å­˜ä¼°ç®—: {format_number(total_memory)}B")
        print(f"  çº¦ {total_memory/(1024**3):.1f} GB")
    
    return {
        'total_params': total_params,
        'activated_params': activated_params,
        'total_flops': total_flops,
        'params_breakdown': {
            'embedding': embedding_params,
            'attention': total_attn_params,
            'moe': total_moe_params,
            'layernorm': layernorm_params
        },
        'flops_breakdown': {
            'embedding': embedding_flops,
            'attention': total_attn_flops,
            'moe': total_moe_flops,
            'output': output_embedding_flops,
            'layernorm': layernorm_flops
        }
    }

def calculate_model_decode_stats(batch_size=1, prefix_length=2048, show_memory=True, gpu_mem_bandwidth_gbps=4000, gpu_tflops_bf16=148, gpu_tflops_fp8=296):
    """
    è®¡ç®—æ¨¡å‹è§£ç é˜¶æ®µçš„FLOPsã€å‚æ•°é‡å’Œè®¿å­˜é‡ï¼Œå¹¶ä¼°ç®—ç†è®ºæ‰§è¡Œæ—¶é—´ã€‚
    å‡è®¾:
    - é™¤äº† self-attention è®¡ç®—å¤–ï¼Œæ‰€æœ‰è®¿å­˜éƒ½ä½¿ç”¨ FP8 (1 byte)
    - Self-attention è®¡ç®—ä½¿ç”¨ BF16 (2 bytes)
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        prefix_length (int): å‰ç¼€é•¿åº¦ï¼ˆåŒ…æ‹¬promptå’Œä¹‹å‰ç”Ÿæˆçš„tokensï¼‰
        show_memory (bool): æ˜¯å¦æ˜¾ç¤ºå†…å­˜ä¼°ç®—
        gpu_mem_bandwidth_gbps (float): GPUå†…å­˜å¸¦å®½ (GB/s)
        gpu_tflops_bf16 (float): GPU BF16è®¡ç®—èƒ½åŠ› (TFLOPS)
        gpu_tflops_fp8 (float): GPU FP8è®¡ç®—èƒ½åŠ› (TFLOPS)
    """
    
    # æ¨¡å‹åŸºæœ¬å‚æ•°
    vocab_size = 151936
    hidden_size = 4096
    num_layers = 94
    
    # Attentionå‚æ•°
    num_q_heads = 64
    num_kv_heads = 4  # GQA (Grouped Query Attention)
    head_dim = 128
    
    # MoEå‚æ•°
    num_experts = 128
    num_activated_experts = 8
    intermediate_size = 1536  # up projectionçš„ä¸€åŠ
    
    # è§£ç é˜¶æ®µï¼šæ¯æ¬¡å¤„ç†1ä¸ªæ–°token
    new_token_len = 1
    bf16_size = 2
    fp8_size = 1
    
    print(f"æ¨¡å‹é…ç½® (è§£ç é˜¶æ®µ):")
    print(f"  è¯æ±‡è¡¨å¤§å°: {format_number(vocab_size)}")
    print(f"  éšè—ç»´åº¦: {format_number(hidden_size)}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  Qå¤´æ•°: {num_q_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"  ä¸“å®¶æ€»æ•°: {num_experts}, æ¿€æ´»ä¸“å®¶æ•°: {num_activated_experts}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, å‰ç¼€é•¿åº¦: {format_number(prefix_length)}")
    print(f"  GPUè§„æ ¼: å†…å­˜å¸¦å®½={gpu_mem_bandwidth_gbps} GB/s, TFLOPS (BF16/FP8)={gpu_tflops_bf16}/{gpu_tflops_fp8}")
    print("\n" + "="*80)
    
    total_params = 0
    total_flops = 0
    total_mem_access = 0
    total_compute_time_us = 0
    total_mem_time_us = 0
    
    # 1. Input Embedding (åªå¤„ç†æ–°token)
    print("\n1. Input Embedding Layer")
    embedding_params = vocab_size * hidden_size
    embedding_flops = batch_size * new_token_len * hidden_size  # lookupæ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
    # è®¿å­˜ä½¿ç”¨ FP8
    embedding_mem_access = (embedding_params * fp8_size) + (batch_size * new_token_len * hidden_size * fp8_size)  # è¯»æƒé‡ + å†™hidden_states
    
    compute_time, mem_time = estimate_time_us(embedding_flops, embedding_mem_access, gpu_tflops_bf16, gpu_mem_bandwidth_gbps)
    print(f"  å‚æ•°é‡: {format_number(embedding_params)}")
    print(f"  FLOPs: {format_number(embedding_flops)}")
    print(f"  è®¿å­˜é‡: {format_number(embedding_mem_access)}B (FP8)")
    print(f"  Time (Compute/Memory): {compute_time:.2f}/{mem_time:.2f} us")
    
    total_params += embedding_params
    total_flops += embedding_flops
    total_mem_access += embedding_mem_access
    total_compute_time_us += compute_time
    total_mem_time_us += mem_time
    
    # 2. Attention Layers (x94)
    print(f"\n2. Attention Layers (x{num_layers})")
    
    # Q projection (åªå¯¹æ–°tokenè®¡ç®—)
    q_proj_params = hidden_size * (num_q_heads * head_dim)
    q_proj_flops = 2 * batch_size * new_token_len * hidden_size * (num_q_heads * head_dim)
    
    # K, V projection (åªå¯¹æ–°tokenè®¡ç®—)
    kv_proj_params = hidden_size * (num_kv_heads * head_dim * 2)
    kv_proj_flops = 2 * batch_size * new_token_len * hidden_size * (num_kv_heads * head_dim * 2)
    
    # Attention computation
    # Q * K^T: 1ä¸ªæ–°queryä¸prefix_lengthä¸ªkeysåšè®¡ç®—
    qk_flops = 2 * batch_size * num_q_heads * new_token_len * prefix_length * head_dim
    # Softmax: å¯¹prefix_lengthé•¿åº¦åšsoftmax
    softmax_flops = batch_size * num_q_heads * new_token_len * prefix_length * 3
    # Attention weights * V: attention weightsä¸prefix_lengthä¸ªvaluesè®¡ç®—
    av_flops = 2 * batch_size * num_q_heads * new_token_len * prefix_length * head_dim
    
    # O projection (åªå¯¹æ–°tokençš„è¾“å‡ºåšprojection)
    o_proj_params = (num_q_heads * head_dim) * hidden_size
    o_proj_flops = 2 * batch_size * new_token_len * (num_q_heads * head_dim) * hidden_size
    
    # æ¯å±‚attentionçš„æ€»è®¡
    attn_params_per_layer = q_proj_params + kv_proj_params + o_proj_params
    attn_flops_per_layer = (q_proj_flops + kv_proj_flops + 
                           qk_flops + softmax_flops + av_flops + o_proj_flops)
    
    # è®¿å­˜é‡è®¡ç®—
    # QKV Projection (FP8)
    qkv_proj_mem_access = (batch_size * new_token_len * hidden_size * fp8_size) + \
                          (batch_size * (num_q_heads + 2 * num_kv_heads) * head_dim * fp8_size) + \
                          (hidden_size * (num_q_heads + 2 * num_kv_heads) * head_dim * fp8_size)
    
    # Attention Score (BF16 - self attention è®¡ç®—ä½¿ç”¨ BF16)
    attn_mem_access = (batch_size * num_q_heads * new_token_len * head_dim * bf16_size) + \
                      (2 * batch_size * prefix_length * num_kv_heads * head_dim * bf16_size) + \
                      (batch_size * num_q_heads * new_token_len * head_dim * bf16_size)  # Q read, KV read, O write
    
    # O Projection (FP8)
    o_proj_mem_access = (batch_size * new_token_len * num_q_heads * head_dim * fp8_size) + \
                        (batch_size * new_token_len * hidden_size * fp8_size) + \
                        (num_q_heads * head_dim * hidden_size * fp8_size)
    
    print(f"  -- æ¯å±‚ --")
    # QKV Proj Time
    qkv_compute_time, qkv_mem_time = estimate_time_us(q_proj_flops + kv_proj_flops, qkv_proj_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  QKV Proj (FP8): FLOPs={format_number(q_proj_flops + kv_proj_flops)}, Mem={format_number(qkv_proj_mem_access)}B, Time(C/M)={qkv_compute_time:.2f}/{qkv_mem_time:.2f} us")
    
    # Attn Score Time (self-attention ä½¿ç”¨ BF16)
    attn_compute_time, attn_mem_time = estimate_time_us(qk_flops + softmax_flops + av_flops, attn_mem_access, gpu_tflops_bf16, gpu_mem_bandwidth_gbps)
    print(f"  Attn Score (BF16): FLOPs={format_number(qk_flops + softmax_flops + av_flops)}, Mem={format_number(attn_mem_access)}B, Time(C/M)={attn_compute_time:.2f}/{attn_mem_time:.2f} us")
    
    # O Proj Time
    o_proj_compute_time, o_proj_mem_time = estimate_time_us(o_proj_flops, o_proj_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  O Proj (FP8): FLOPs={format_number(o_proj_flops)}, Mem={format_number(o_proj_mem_access)}B, Time(C/M)={o_proj_compute_time:.2f}/{o_proj_mem_time:.2f} us")
    
    total_attn_params = attn_params_per_layer * num_layers
    total_attn_flops = attn_flops_per_layer * num_layers
    total_attn_mem_access = (qkv_proj_mem_access + attn_mem_access + o_proj_mem_access) * num_layers
    
    # æ¯å±‚æ—¶é—´
    attn_compute_time_per_layer = qkv_compute_time + attn_compute_time + o_proj_compute_time    
    attn_mem_time_per_layer = qkv_mem_time + attn_mem_time + o_proj_mem_time
    total_attn_compute_time_us = attn_compute_time_per_layer * num_layers
    total_attn_mem_time_us = attn_mem_time_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(attn_params_per_layer)}")
    print(f"  æ¯å±‚FLOPs: {format_number(attn_flops_per_layer)}")
    print(f"  æ¯å±‚æ—¶é—´(C/M): {qkv_compute_time:.2f}/{qkv_mem_time:.2f} us, {attn_compute_time:.2f}/{attn_mem_time:.2f} us, {o_proj_compute_time:.2f}/{o_proj_mem_time:.2f} us")     
    print(f"  æ€»å‚æ•°é‡: {format_number(total_attn_params)}")
    print(f"  æ€»FLOPs: {format_number(total_attn_flops)}")
    print(f"  æ€»è®¿å­˜é‡: {format_number(total_attn_mem_access)}B")
    print(f"  æ€»æ—¶é—´(C/M): {total_attn_compute_time_us:.2f}/{total_attn_mem_time_us:.2f} us")
    
    total_params += total_attn_params
    total_flops += total_attn_flops
    total_mem_access += total_attn_mem_access
    total_compute_time_us += total_attn_compute_time_us
    total_mem_time_us += total_attn_mem_time_us
    
    # 3. MoE Layers (x94)
    print(f"\n3. MoE Layers (x{num_layers})")
    
    # Up projection (gate + up) - åªå¯¹æ–°tokenè®¡ç®—
    up_params_per_expert = hidden_size * (intermediate_size * 2)
    total_up_params = up_params_per_expert * num_experts
    activated_up_params = up_params_per_expert * num_activated_experts
    up_flops = 2 * batch_size * new_token_len * hidden_size * (intermediate_size * 2) * num_activated_experts
    
    # Down projection - åªå¯¹æ–°tokenè®¡ç®—
    down_params_per_expert = intermediate_size * hidden_size
    total_down_params = down_params_per_expert * num_experts
    activated_down_params = down_params_per_expert * num_activated_experts
    down_flops = 2 * batch_size * new_token_len * intermediate_size * hidden_size * num_activated_experts
    
    # æ¿€æ´»å‡½æ•° (SiLU + ä¹˜æ³•) - åªå¯¹æ–°tokenè®¡ç®—
    activation_flops = batch_size * new_token_len * intermediate_size * num_activated_experts * 3
    
    # è·¯ç”±ç½‘ç»œ - åªå¯¹æ–°tokenè®¡ç®—
    router_params = hidden_size * num_experts
    router_flops = 2 * batch_size * new_token_len * hidden_size * num_experts
    
    # æ¯å±‚MoEçš„æ€»è®¡
    moe_params_per_layer = total_up_params + total_down_params + router_params
    moe_flops_per_layer = up_flops + down_flops + activation_flops + router_flops
    
    # è®¿å­˜é‡è®¡ç®— (FP8)
    # Routerè®¡ç®—
    router_mem_access = (batch_size * new_token_len * hidden_size * fp8_size) + \
                        (batch_size * new_token_len * num_experts * fp8_size) + \
                        (hidden_size * num_experts * fp8_size)
    
    # Gate/Up Projection (å¯¹æ¿€æ´»çš„8ä¸ªä¸“å®¶)
    up_mem_access = (batch_size * new_token_len * hidden_size * fp8_size) + \
                    (batch_size * new_token_len * intermediate_size * 2 * num_activated_experts * fp8_size) + \
                    (hidden_size * intermediate_size * 2 * num_activated_experts * fp8_size)
    
    # Down Projection (å¯¹æ¿€æ´»çš„8ä¸ªä¸“å®¶)
    down_mem_access = (batch_size * new_token_len * intermediate_size * num_activated_experts * fp8_size) + \
                      (batch_size * new_token_len * hidden_size * fp8_size) + \
                      (intermediate_size * hidden_size * num_activated_experts * fp8_size)
    
    print(f"  -- æ¯å±‚ --")
    router_compute_time, router_mem_time = estimate_time_us(router_flops, router_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  Router (FP8): FLOPs={format_number(router_flops)}, Mem={format_number(router_mem_access)}B, Time(C/M)={router_compute_time:.2f}/{router_mem_time:.2f} us")
    
    up_compute_time, up_mem_time = estimate_time_us(up_flops, up_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  Gate/Up Proj (FP8): FLOPs={format_number(up_flops)}, Mem={format_number(up_mem_access)}B, Time(C/M)={up_compute_time:.2f}/{up_mem_time:.2f} us")
    
    down_compute_time, down_mem_time = estimate_time_us(down_flops, down_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  Down Proj (FP8): FLOPs={format_number(down_flops)}, Mem={format_number(down_mem_access)}B, Time(C/M)={down_compute_time:.2f}/{down_mem_time:.2f} us")
    
    # æ³¨æ„ï¼šactivation FLOPs è™½ç„¶å­˜åœ¨ä½†è®¿å­˜å¿½ç•¥ä¸è®¡
    print(f"  Activation: FLOPs={format_number(activation_flops)} (è®¿å­˜å¿½ç•¥ä¸è®¡)")
    
    # æ¯å±‚æ—¶é—´
    moe_compute_time_per_layer = router_compute_time + up_compute_time + down_compute_time
    moe_mem_time_per_layer = router_mem_time + up_mem_time + down_mem_time
    total_moe_params = moe_params_per_layer * num_layers
    total_moe_flops = moe_flops_per_layer * num_layers
    total_moe_mem_access = (router_mem_access + up_mem_access + down_mem_access) * num_layers
    total_moe_compute_time_us = moe_compute_time_per_layer * num_layers
    total_moe_mem_time_us = moe_mem_time_per_layer * num_layers
    
    print(f"  æ¯å±‚å‚æ•°é‡: {format_number(moe_params_per_layer)}")
    print(f"  æ¯å±‚æ¿€æ´»å‚æ•°é‡: {format_number(activated_up_params + activated_down_params)}")
    print(f"  æ¯å±‚FLOPs: {format_number(moe_flops_per_layer)}")
    print(f"  æ¯å±‚æ—¶é—´(C/M): {router_compute_time:.2f}/{router_mem_time:.2f} us, {up_compute_time:.2f}/{up_mem_time:.2f} us, {down_compute_time:.2f}/{down_mem_time:.2f} us")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_moe_params)}")
    print(f"  æ€»FLOPs: {format_number(total_moe_flops)}")
    print(f"  æ€»è®¿å­˜é‡: {format_number(total_moe_mem_access)}B")
    print(f"  æ€»æ—¶é—´(C/M): {total_moe_compute_time_us:.2f}/{total_moe_mem_time_us:.2f} us")
    
    total_params += total_moe_params
    total_flops += total_moe_flops
    total_mem_access += total_moe_mem_access
    total_compute_time_us += total_moe_compute_time_us
    total_mem_time_us += total_moe_mem_time_us
    
    # 4. Output Embedding (åªå¯¹æ–°tokenè®¡ç®—)
    print(f"\n4. Output Embedding Layer")
    output_embedding_params = 0  # é€šå¸¸å…±äº«æƒé‡
    output_embedding_flops = 2 * batch_size * new_token_len * hidden_size * vocab_size
    # è®¿å­˜ä½¿ç”¨ FP8
    output_mem_access = (batch_size * new_token_len * hidden_size * fp8_size) + \
                        (batch_size * new_token_len * vocab_size * fp8_size) + \
                        (hidden_size * vocab_size * fp8_size)
    
    compute_time, mem_time = estimate_time_us(output_embedding_flops, output_mem_access, gpu_tflops_fp8, gpu_mem_bandwidth_gbps)
    print(f"  å‚æ•°é‡: {format_number(output_embedding_params)} (å…±äº«æƒé‡)")
    print(f"  FLOPs: {format_number(output_embedding_flops)}")
    print(f"  è®¿å­˜é‡: {format_number(output_mem_access)}B (FP8)")
    print(f"  Time (Compute/Memory): {compute_time:.2f}/{mem_time:.2f} us")
    
    total_flops += output_embedding_flops
    total_mem_access += output_mem_access
    total_compute_time_us += compute_time
    total_mem_time_us += mem_time
    
    # 5. Layer Norm (åªå¯¹æ–°tokenè®¡ç®—)
    print(f"\n5. Layer Normalization")
    # æ¯å±‚æœ‰2ä¸ªLayerNorm: attentionåå’ŒMoEå
    layernorm_params = hidden_size * 2 * num_layers * 2  # scaleå’Œbias
    layernorm_flops = batch_size * new_token_len * hidden_size * 5 * 2 * num_layers
    
    print(f"  å‚æ•°é‡: {format_number(layernorm_params)}")
    print(f"  FLOPs: {format_number(layernorm_flops)}")
    
    total_params += layernorm_params
    total_flops += layernorm_flops
    
    # 6. å…¶ä»–å†…å­˜è®¿é—® (Norm, Residual, KV Cache)
    print("\n6. å…¶ä»–å†…å­˜è®¿é—®")
    
    # Normå’ŒResiduals (FP8)
    norm_resid_mem_access = (batch_size * new_token_len * hidden_size * fp8_size * 2 * 2 * 2 * num_layers) + \
                            (layernorm_params * fp8_size)
    
    # KV Cache (BF16 - self attention ç›¸å…³)
    kv_cache_read_volume = batch_size * prefix_length * num_kv_heads * 2 * head_dim * bf16_size * num_layers
    kv_cache_write_volume = batch_size * new_token_len * num_kv_heads * 2 * head_dim * bf16_size * num_layers
    total_kv_cache_access = kv_cache_read_volume + kv_cache_write_volume
    
    _, mem_time_norm = estimate_time_us(0, norm_resid_mem_access, gpu_tflops_bf16, gpu_mem_bandwidth_gbps)
    _, mem_time_kv = estimate_time_us(0, total_kv_cache_access, gpu_tflops_bf16, gpu_mem_bandwidth_gbps)
    print(f"  Norm/Residuals: Mem={format_number(norm_resid_mem_access)}B (FP8), Time (Memory): {mem_time_norm:.2f} us")
    print(f"  KV Cache R/W: Mem={format_number(total_kv_cache_access)}B (BF16), Time (Memory): {mem_time_kv:.2f} us")
    
    total_mem_access += norm_resid_mem_access + total_kv_cache_access
    total_mem_time_us += mem_time_norm + mem_time_kv
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»è®¡ (è§£ç é˜¶æ®µ - ç”Ÿæˆ1ä¸ªtoken):")
    print(f"  æ€»å‚æ•°é‡: {format_number(total_params)}")
    print(f"  æ€»FLOPs: {format_number(total_flops)}")
    print(f"  æ€»è®¿å­˜é‡: {format_number(total_mem_access)}B")
    print("-" * 40)
    print(f"  æ€»ä¼°ç®—æ—¶é—´ (Compute-bound): {total_compute_time_us:.2f} us")
    print(f"  æ€»ä¼°ç®—æ—¶é—´ (Memory-bound): {total_mem_time_us:.2f} us")
    print(f"  é¢„ä¼°ç“¶é¢ˆ: {'Memory' if total_mem_time_us > total_compute_time_us else 'Compute'}")
    
    # æ¿€æ´»å‚æ•°é‡ï¼ˆå®é™…ä½¿ç”¨çš„å‚æ•°ï¼‰
    activated_params = (embedding_params + 
                       total_attn_params + 
                       (activated_up_params + activated_down_params + router_params) * num_layers +
                       layernorm_params)
    
    print(f"\n  æ¿€æ´»å‚æ•°é‡: {format_number(activated_params)}")
    print(f"  å‚æ•°åˆ©ç”¨ç‡: {activated_params/total_params*100:.1f}%")
    
    # è¯¦ç»†å‚æ•°åˆ†è§£
    print(f"\nğŸ“ˆ å‚æ•°åˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_params)} ({embedding_params/total_params*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_params)} ({total_attn_params/total_params*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_params)} ({total_moe_params/total_params*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_params)} ({layernorm_params/total_params*100:.1f}%)")
    
    # FLOPsåˆ†è§£
    print(f"\nâš¡ FLOPsåˆ†è§£:")
    print(f"  Embedding: {format_number(embedding_flops)} ({embedding_flops/total_flops*100:.1f}%)")
    print(f"  Attention: {format_number(total_attn_flops)} ({total_attn_flops/total_flops*100:.1f}%)")
    print(f"  MoE: {format_number(total_moe_flops)} ({total_moe_flops/total_flops*100:.1f}%)")
    print(f"  Output: {format_number(output_embedding_flops)} ({output_embedding_flops/total_flops*100:.1f}%)")
    print(f"  LayerNorm: {format_number(layernorm_flops)} ({layernorm_flops/total_flops*100:.1f}%)")
    
    # å†…å­˜å ç”¨ä¼°ç®—
    if show_memory:
        print(f"\nğŸ’¾ å†…å­˜å ç”¨ä¼°ç®—:")
        print("-" * 40)
        
        # æ¨¡å‹æƒé‡å†…å­˜ (FP8)
        gemm_params = total_attn_params + total_moe_params + vocab_size * hidden_size  # output layer
        non_gemm_params = embedding_params + layernorm_params
        model_memory = (gemm_params + non_gemm_params) * fp8_size  # å…¨éƒ¨ä½¿ç”¨ FP8
        
        # æ¿€æ´»å†…å­˜ (FP8) - è§£ç é˜¶æ®µåªå¤„ç†1ä¸ªtokenï¼Œå†…å­˜éœ€æ±‚å¤§å¤§é™ä½
        attention_activation = batch_size * new_token_len * hidden_size * 8
        moe_activation = batch_size * new_token_len * intermediate_size * 2 * num_activated_experts
        activation_memory = (attention_activation + moe_activation) * num_layers * fp8_size
        
        # KV Cache (BF16) - å­˜å‚¨åˆ°prefix_lengthçš„æ‰€æœ‰å†å²
        kv_cache_memory = batch_size * prefix_length * num_kv_heads * head_dim * 2 * num_layers * bf16_size
        
        total_memory = model_memory + activation_memory + kv_cache_memory
        
        print(f"  æ¨¡å‹æƒé‡å†…å­˜ (FP8): {format_number(model_memory/1024**3)} GB")
        print(f"  æ¿€æ´»å†…å­˜ (FP8): {format_number(activation_memory/1024**3)} GB")
        print(f"  KV Cacheå†…å­˜ (BF16): {format_number(kv_cache_memory/1024**3)} GB")
        print(f"  æ€»å†…å­˜å ç”¨: {format_number(total_memory/1024**3)} GB")
        
        # ä¸é¢„å¡«å……é˜¶æ®µçš„å¯¹æ¯”
        print(f"\nğŸ”„ è§£ç  vs é¢„å¡«å……å¯¹æ¯”:")
        print(f"  æ¿€æ´»å†…å­˜å‡å°‘: ~{new_token_len/prefix_length*100:.1f}% (å•token vs {format_number(prefix_length)}tokens)")
        print(f"  KV Cache: ç›¸åŒå¤§å° (éœ€è¦ä¿å­˜å®Œæ•´å†å²)")
    
    return {
        'total_params': total_params,
        'activated_params': activated_params,
        'total_flops': total_flops,
        'prefix_length': prefix_length,
        'params_breakdown': {
            'embedding': embedding_params,
            'attention': total_attn_params,
            'moe': total_moe_params,
            'layernorm': layernorm_params
        },
        'flops_breakdown': {
            'embedding': embedding_flops,
            'attention': total_attn_flops,
            'moe': total_moe_flops,
            'output': output_embedding_flops,
            'layernorm': layernorm_flops
        }
    }

def compare_sequence_lengths():
    """æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„è®¡ç®—é‡"""
    print("\n" + "="*80)
    print("ğŸ” ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„FLOPså¯¹æ¯”:")
    print("="*80)
    
    seq_lengths = [512, 1024, 2048, 4096, 8192]
    
    print(f"{'åºåˆ—é•¿åº¦':<10} {'æ€»FLOPs':<12} {'Attention FLOPs':<15} {'MoE FLOPs':<12}")
    print("-" * 55)
    
    for seq_len in seq_lengths:
        stats = calculate_model_prefill_stats(batch_size=1, seq_len=seq_len, show_memory=False)
        attn_flops = stats['flops_breakdown']['attention']
        moe_flops = stats['flops_breakdown']['moe']
        total_flops = stats['total_flops']
        
        print(f"{seq_len:<10} {format_number(total_flops):<12} {format_number(attn_flops):<15} {format_number(moe_flops):<12}")

def compare_prefill_vs_decode():
    """æ¯”è¾ƒé¢„å¡«å……é˜¶æ®µå’Œè§£ç é˜¶æ®µçš„è®¡ç®—é‡"""
    print("\n" + "="*80)
    print("ğŸ”„ é¢„å¡«å…… vs è§£ç é˜¶æ®µå¯¹æ¯”:")
    print("="*80)
    
    batch_size = 96
    seq_len = 6000
    prefix_length = 6000
    
    print(f"å¯¹æ¯”é…ç½®: batch_size={batch_size}, seq_len={seq_len}, prefix_length={prefix_length}")
    print("-" * 80)
    
    # é¢„å¡«å……é˜¶æ®µç»Ÿè®¡
    prefill_stats = calculate_model_prefill_stats(batch_size=batch_size, seq_len=seq_len, show_memory=False)
    
    # è§£ç é˜¶æ®µç»Ÿè®¡  
    decode_stats = calculate_model_decode_stats(batch_size=batch_size, prefix_length=prefix_length, show_memory=False)
    
    print(f"\nğŸ“Š FLOPså¯¹æ¯”:")
    print(f"{'é˜¶æ®µ':<12} {'æ€»FLOPs':<15} {'Attention':<15} {'MoE':<15} {'Output':<15}")
    print("-" * 75)
    
    prefill_total = prefill_stats['total_flops']
    prefill_attn = prefill_stats['flops_breakdown']['attention']  
    prefill_moe = prefill_stats['flops_breakdown']['moe']
    prefill_output = prefill_stats['flops_breakdown']['output']
    
    decode_total = decode_stats['total_flops']
    decode_attn = decode_stats['flops_breakdown']['attention']
    decode_moe = decode_stats['flops_breakdown']['moe'] 
    decode_output = decode_stats['flops_breakdown']['output']
    
    print(f"{'é¢„å¡«å……':<12} {format_number(prefill_total):<15} {format_number(prefill_attn):<15} {format_number(prefill_moe):<15} {format_number(prefill_output):<15}")
    print(f"{'è§£ç ':<12} {format_number(decode_total):<15} {format_number(decode_attn):<15} {format_number(decode_moe):<15} {format_number(decode_output):<15}")
    print(f"{'æ¯”ç‡':<12} {decode_total/prefill_total:.3f}{'x':<14} {decode_attn/prefill_attn:.3f}{'x':<14} {decode_moe/prefill_moe:.3f}{'x':<14} {decode_output/prefill_output:.3f}{'x':<14}")
    
    print(f"\nğŸ’¡ å…³é”®è§‚å¯Ÿ:")
    print(f"  - è§£ç é˜¶æ®µæ€»FLOPsæ˜¯é¢„å¡«å……é˜¶æ®µçš„ {decode_total/prefill_total:.2f}x")
    print(f"  - Attentionè®¡ç®—åœ¨è§£ç é˜¶æ®µæ˜¾è‘—é™ä½ ({decode_attn/prefill_attn:.3f}x)")
    print(f"  - MoEè®¡ç®—å¤§å¹…é™ä½ ({decode_moe/prefill_moe:.3f}x)")
    print(f"  - æ¯ä¸ªtokençš„è§£ç æˆæœ¬è¿œä½äºé¢„å¡«å……é˜¶æ®µ")

if __name__ == "__main__":
    print("ğŸš€ Qwen3-235B æ¨¡å‹åˆ†æ")
    print("="*80)
    
    # H100 SXM5 specs as reference
    GPU_MEM_BANDWIDTH_GBPS = 4000
    GPU_TFLOPS_BF16 = 148
    GPU_TFLOPS_FP8 = 296
    
    # # è®¡ç®—é¢„å¡«å……é˜¶æ®µç»Ÿè®¡ä¿¡æ¯
    # print("\nğŸ”¥ é¢„å¡«å……é˜¶æ®µåˆ†æ:")
    # calculate_model_prefill_stats(batch_size=2, seq_len=4096, show_memory=True)
    
    # è®¡ç®—è§£ç é˜¶æ®µç»Ÿè®¡ä¿¡æ¯ï¼ˆå¸¦æ—¶é—´ä¼°ç®—ï¼‰
    print("\n\n" + "="*80)
    print("\nğŸ¯ è§£ç é˜¶æ®µåˆ†æ:")
    calculate_model_decode_stats(
        batch_size=96, 
        prefix_length=6000, 
        show_memory=True,
        gpu_mem_bandwidth_gbps=GPU_MEM_BANDWIDTH_GBPS,
        gpu_tflops_bf16=GPU_TFLOPS_BF16,
        gpu_tflops_fp8=GPU_TFLOPS_FP8
    )
    
    # # é¢„å¡«å……vsè§£ç å¯¹æ¯”
    # compare_prefill_vs_decode()
    
    # æ¯”è¾ƒä¸åŒåºåˆ—é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    # compare_sequence_lengths() 