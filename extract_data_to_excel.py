#!/usr/bin/env python3
"""
DeepEP 16èŠ‚ç‚¹è·¨èŠ‚ç‚¹æµ‹è¯•æ—¥å¿—åˆ†æè„šæœ¬
ä»rank_16.logä¸­æå–RDMAå’ŒNVLæ€§èƒ½æ•°æ®å¹¶ç”ŸæˆExcel/CSVæŠ¥å‘Š
"""

import re
import os
import csv
from typing import Dict, List, Tuple, Optional
import argparse

def parse_log_file(log_path: str) -> List[Dict]:
    """
    è§£ærank_16.logæ–‡ä»¶ï¼Œæå–è·¨èŠ‚ç‚¹æ€§èƒ½æ•°æ®
    
    Args:
        log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«æµ‹è¯•æ•°æ®çš„å­—å…¸åˆ—è¡¨
    """
    if not os.path.exists(log_path):
        print(f"é”™è¯¯: æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨")
        return []
    
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    results = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¯ä¸ªé…ç½®å—
    # æ ¼å¼: [config] ... [tuning] Best dispatch ... [tuning] Best combine ...
    config_pattern = r'\[config\] num_tokens=(\d+), hidden=(\d+), num_topk=(\d+) ,num_experts=(\d+)'
    
    # æ‰¾åˆ°æ‰€æœ‰é…ç½®è¡Œ
    config_matches = list(re.finditer(config_pattern, content))
    
    for i, config_match in enumerate(config_matches):
        # æå–é…ç½®ä¿¡æ¯
        num_tokens = int(config_match.group(1))
        hidden = int(config_match.group(2))
        num_topk = int(config_match.group(3))
        num_experts = int(config_match.group(4))
        
        # ç¡®å®šå½“å‰é…ç½®å—çš„èŒƒå›´
        start_pos = config_match.end()
        if i + 1 < len(config_matches):
            end_pos = config_matches[i + 1].start()
            section_content = content[start_pos:end_pos]
        else:
            section_content = content[start_pos:]
        
        # æå–dispatchæ€§èƒ½æ•°æ®
        # [tuning] Best dispatch (FP8): SMs 36, NVL chunk 8, RDMA chunk 8, transmit: 27.54 us, notify: 31.65 us, BW: 1.07 GB/s (RDMA), 2.95 GB/s (NVL)
        dispatch_pattern = r'\[tuning\] Best dispatch \(FP8\): SMs (\d+), NVL chunk (\d+), RDMA chunk (\d+), transmit: ([\d.]+) us, notify: ([\d.]+) us, BW: ([\d.]+) GB/s \(RDMA\), ([\d.]+) GB/s \(NVL\)'
        dispatch_match = re.search(dispatch_pattern, section_content)
        
        # æå–combineæ€§èƒ½æ•°æ®
        # [tuning] Best combine: SMs 36, NVL chunk 7, RDMA chunk 16, transmit: 55.25 us, notify: 30.98 us, BW: 1.04 GB/s (RDMA), 2.85 GB/s (NVL)
        combine_pattern = r'\[tuning\] Best combine: SMs (\d+), NVL chunk (\d+), RDMA chunk (\d+), transmit: ([\d.]+) us, notify: ([\d.]+) us, BW: ([\d.]+) GB/s \(RDMA\), ([\d.]+) GB/s \(NVL\)'
        combine_match = re.search(combine_pattern, section_content)
        
        # æ„å»ºç»“æœå­—å…¸
        result = {
            'num_tokens': num_tokens,
            'hidden': hidden,
            'num_topk': num_topk,
            'num_experts': num_experts,
        }
        
        # æ·»åŠ dispatchæ•°æ®
        if dispatch_match:
            result.update({
                'dispatch_sms': int(dispatch_match.group(1)),
                'dispatch_nvl_chunk': int(dispatch_match.group(2)),
                'dispatch_rdma_chunk': int(dispatch_match.group(3)),
                'dispatch_transmit_us': float(dispatch_match.group(4)),
                'dispatch_notify_us': float(dispatch_match.group(5)),
                'dispatch_rdma_bandwidth_gbps': float(dispatch_match.group(6)),
                'dispatch_nvl_bandwidth_gbps': float(dispatch_match.group(7)),
            })
        
        # æ·»åŠ combineæ•°æ®
        if combine_match:
            result.update({
                'combine_sms': int(combine_match.group(1)),
                'combine_nvl_chunk': int(combine_match.group(2)),
                'combine_rdma_chunk': int(combine_match.group(3)),
                'combine_transmit_us': float(combine_match.group(4)),
                'combine_notify_us': float(combine_match.group(5)),
                'combine_rdma_bandwidth_gbps': float(combine_match.group(6)),
                'combine_nvl_bandwidth_gbps': float(combine_match.group(7)),
            })
        
        
        results.append(result)
    
    return results

def parse_ll_log_file(log_path: str) -> List[Dict]:
    """
    è§£æ *ll.log æ–‡ä»¶ï¼Œæå–ä¸¤ç±»è¡Œå„ç”Ÿæˆä¸€è¡Œï¼š
    1) return_recv_hook=True çš„ send/recv time è¡Œ
    2) return_recv_hook=False çš„ bandwidth/avg_t è¡Œ
    æ¯åŒ¹é…ä¸€è¡Œå³è¿”å›ä¸€æ¡è®°å½•ã€‚
    """
    if not os.path.exists(log_path):
        print(f"é”™è¯¯: æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨")
        return []

    results: List[Dict] = []

    # é¢„ç¼–è¯‘æ­£åˆ™
    alloc_pattern = re.compile(r"Allocating buffer size: ([\d.]+) MB ...")
    timing_pattern = re.compile(
        r"\[rank \d+\] num_tokens=(\d+), hidden=(\d+), num_experts=(\d+), num_topk=(\d+), return_recv_hook=True "
        r"Dispatch send/recv time: ([\d.]+) \+ ([\d.]+) us \| Combine send/recv time: ([\d.]+) \+ ([\d.]+) us"
    )
    bw_pattern = re.compile(
        r"\[rank \d+\] num_tokens=(\d+), hidden=(\d+), num_experts=(\d+), num_topk=(\d+), return_recv_hook=False "
        r"Dispatch bandwidth: ([\d.]+) GB/s, avg_t=([\d.]+) us \| Combine bandwidth: ([\d.]+) GB/s, avg_t=([\d.]+) us"
    )

    current_alloc_mb: Optional[float] = None

    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # æ•è·æœ€è¿‘çš„ buffer å¤§å°
            alloc_m = alloc_pattern.search(line)
            if alloc_m:
                try:
                    current_alloc_mb = float(alloc_m.group(1))
                except ValueError:
                    current_alloc_mb = None
                continue

            # åŒ¹é… timing è¡Œï¼ˆreturn_recv_hook=Trueï¼‰
            m1 = timing_pattern.search(line)
            if m1:
                try:
                    num_tokens = int(m1.group(1))
                    hidden = int(m1.group(2))
                    num_experts = int(m1.group(3))
                    num_topk = int(m1.group(4))
                    dispatch_send = float(m1.group(5))
                    dispatch_recv = float(m1.group(6))
                    combine_send = float(m1.group(7))
                    combine_recv = float(m1.group(8))
                except ValueError:
                    continue

                row: Dict = {
                    'num_tokens': num_tokens,
                    'hidden': hidden,
                    'num_topk': num_topk,
                    'num_experts': num_experts,
                    'return_recv_hook': True,
                    'dispatch_transmit_us': dispatch_send,
                    'dispatch_notify_us': dispatch_recv,
                    'combine_transmit_us': combine_send,
                    'combine_notify_us': combine_recv,
                }
                if current_alloc_mb is not None:
                    row['data_size_mb'] = current_alloc_mb
                results.append(row)
                continue

            # åŒ¹é… bandwidth è¡Œï¼ˆreturn_recv_hook=Falseï¼‰
            m2 = bw_pattern.search(line)
            if m2:
                try:
                    num_tokens = int(m2.group(1))
                    hidden = int(m2.group(2))
                    num_experts = int(m2.group(3))
                    num_topk = int(m2.group(4))
                    dispatch_bw = float(m2.group(5))
                    dispatch_avg_t = float(m2.group(6))
                    combine_bw = float(m2.group(7))
                    combine_avg_t = float(m2.group(8))
                except ValueError:
                    continue

                row2: Dict = {
                    'num_tokens': num_tokens,
                    'hidden': hidden,
                    'num_topk': num_topk,
                    'num_experts': num_experts,
                    'return_recv_hook': False,
                    'dispatch_bandwidth_gbps': dispatch_bw,
                    'dispatch_avg_t_us': dispatch_avg_t,
                    'combine_bandwidth_gbps': combine_bw,
                    'combine_avg_t_us': combine_avg_t,
                }
                if current_alloc_mb is not None:
                    row2['data_size_mb'] = current_alloc_mb
                results.append(row2)
                continue

    return results

def collect_log_files(log_dir: str) -> List[str]:
    """
    ä»…æ”¶é›†æŒ‡å®šç›®å½•ä¸‹(ä¸é€’å½’)çš„ .log æ–‡ä»¶
    """
    if not os.path.isdir(log_dir):
        return []
    log_files: List[str] = []
    for name in os.listdir(log_dir):
        path = os.path.join(log_dir, name)
        if os.path.isfile(path) and name.endswith('.log'):
            log_files.append(path)
    return sorted(log_files)

def _sanitize_sheet_name(name: str) -> str:
    """
    æ¸…ç†Excelå·¥ä½œè¡¨åç§°ï¼ˆå»é™¤éæ³•å­—ç¬¦å¹¶æˆªæ–­è‡³31å­—ç¬¦ï¼‰ã€‚
    """
    invalid = set(':\\/?*[]')
    clean = ''.join('_' if ch in invalid else ch for ch in name)
    if not clean:
        clean = 'sheet'
    return clean[:31]

def create_csv_report(all_data: List[Dict], output_path: str):
    """
    åˆ›å»ºCSVæŠ¥å‘Š
    
    Args:
        all_data: æ‰€æœ‰æµ‹è¯•æ•°æ®
        output_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
    """
    if not all_data:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
        return
    
    # å®šä¹‰é‡è¦åˆ—çš„é¡ºåº
    priority_columns = [
        'num_tokens', 'hidden', 'num_topk', 'num_experts',
        'data_size_mb', 'total_time_us', 'total_throughput_gbps',
        
        # Dispatch æŒ‡æ ‡
        'dispatch_sms', 'dispatch_nvl_chunk', 'dispatch_rdma_chunk',
        'dispatch_transmit_us', 'dispatch_notify_us',
        'dispatch_rdma_bandwidth_gbps', 'dispatch_nvl_bandwidth_gbps',
        
        # Combine æŒ‡æ ‡
        'combine_sms', 'combine_nvl_chunk', 'combine_rdma_chunk',
        'combine_transmit_us', 'combine_notify_us',
        'combine_rdma_bandwidth_gbps', 'combine_nvl_bandwidth_gbps',
        
        # æ±‡æ€»æŒ‡æ ‡
        'avg_rdma_bandwidth_gbps', 'avg_nvl_bandwidth_gbps',
        'total_transmit_time_us', 'total_notify_time_us',
    ]
    
    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„åˆ—å
    all_columns = set()
    for data in all_data:
        all_columns.update(data.keys())
    
    # æ’åºåˆ—å
    sorted_columns = []
    for col in priority_columns:
        if col in all_columns:
            sorted_columns.append(col)
            all_columns.remove(col)
    
    # æ·»åŠ å‰©ä½™çš„åˆ—
    sorted_columns.extend(sorted(all_columns))
    
    # å†™å…¥CSVæ–‡ä»¶
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=sorted_columns)
        writer.writeheader()
        
        # æŒ‰num_tokensæ’åºåå†™å…¥æ•°æ®
        sorted_data = sorted(all_data, key=lambda x: x.get('num_tokens', 0))
        for row in sorted_data:
            # å°†æ•°å€¼å››èˆäº”å…¥åˆ°4ä½å°æ•°
            rounded_row = {}
            for k, v in row.items():
                if isinstance(v, float):
                    rounded_row[k] = round(v, 4)
                else:
                    rounded_row[k] = v
            writer.writerow(rounded_row)
    
    print(f"âœ… CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")

def create_summary_report(all_data: List[Dict], output_dir: str):
    """
    åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
    """
    if not all_data:
        return
    
    summary_path = os.path.join(output_dir, "internode_summary.txt")
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("DeepEP 16èŠ‚ç‚¹è·¨èŠ‚ç‚¹æµ‹è¯•æ€§èƒ½æ±‡æ€»\n")
        f.write("=" * 50 + "\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        f.write(f"æ€»æµ‹è¯•é…ç½®: {len(all_data)}\n")
        
        # Tokenæ•°èŒƒå›´
        token_values = [d.get('num_tokens', 0) for d in all_data]
        f.write(f"Tokenæ•°èŒƒå›´: {min(token_values)} - {max(token_values)}\n")
        f.write(f"ä¸“å®¶æ•°: {all_data[0].get('num_experts', 'N/A')}\n")
        f.write(f"éšè—å±‚å¤§å°: {all_data[0].get('hidden', 'N/A')}\n\n")
        
        # æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
        metrics = [
            ('dispatch_rdma_bandwidth_gbps', 'Dispatch RDMAå¸¦å®½', 'GB/s'),
            ('dispatch_nvl_bandwidth_gbps', 'Dispatch NVLå¸¦å®½', 'GB/s'),
            ('combine_rdma_bandwidth_gbps', 'Combine RDMAå¸¦å®½', 'GB/s'),
            ('combine_nvl_bandwidth_gbps', 'Combine NVLå¸¦å®½', 'GB/s'),
            ('avg_rdma_bandwidth_gbps', 'å¹³å‡RDMAå¸¦å®½', 'GB/s'),
            ('avg_nvl_bandwidth_gbps', 'å¹³å‡NVLå¸¦å®½', 'GB/s'),
            ('total_throughput_gbps', 'æ•´ä½“ååç‡', 'GB/s'),
            ('dispatch_transmit_us', 'Dispatchä¼ è¾“æ—¶é—´', 'Î¼s'),
            ('combine_transmit_us', 'Combineä¼ è¾“æ—¶é—´', 'Î¼s'),
            ('total_time_us', 'æ€»æ—¶é—´', 'Î¼s'),
        ]
        
        f.write("æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡:\n")
        f.write("-" * 30 + "\n")
        
        for metric_key, metric_name, unit in metrics:
            values = [d.get(metric_key, 0) for d in all_data if metric_key in d]
            if values:
                f.write(f"{metric_name}:\n")
                f.write(f"  å¹³å‡: {sum(values)/len(values):.4f} {unit}\n")
                f.write(f"  æœ€å¤§: {max(values):.4f} {unit}\n")
                f.write(f"  æœ€å°: {min(values):.4f} {unit}\n")
                f.write("\n")
        
        # é…ç½®ä¼˜åŒ–ä¿¡æ¯
        f.write("æœ€ä¼˜é…ç½®ä¿¡æ¯:\n")
        f.write("-" * 30 + "\n")
        
        for data in all_data:
            tokens = data.get('num_tokens', 0)
            f.write(f"Token={tokens}:\n")
            f.write(f"  Dispatch: SMs={data.get('dispatch_sms', 'N/A')}, "
                   f"NVL chunk={data.get('dispatch_nvl_chunk', 'N/A')}, "
                   f"RDMA chunk={data.get('dispatch_rdma_chunk', 'N/A')}\n")
            f.write(f"  Combine: SMs={data.get('combine_sms', 'N/A')}, "
                   f"NVL chunk={data.get('combine_nvl_chunk', 'N/A')}, "
                   f"RDMA chunk={data.get('combine_rdma_chunk', 'N/A')}\n")
            if 'total_throughput_gbps' in data:
                f.write(f"  æ•´ä½“ååç‡: {data['total_throughput_gbps']:.2f} GB/s\n")
            f.write("\n")
    
    print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_path}")

def try_create_excel_report(all_data: List[Dict], output_path: str):
    """
    å°è¯•åˆ›å»ºExcelæŠ¥å‘Šï¼ˆå¦‚æœæœ‰openpyxlçš„è¯ï¼‰
    """
    try:
        from openpyxl import Workbook
    except ImportError:
        print("âŒ æœªå®‰è£…openpyxlï¼Œæ— æ³•ç”ŸæˆExcelã€‚è¯·å®‰è£…åé‡è¯•: pip install openpyxl")
        return False

    # ç»„è£…åˆ—å¤´ï¼ˆä¼˜å…ˆé‡è¦åˆ—ï¼‰
    priority_columns = [
        'num_tokens', 'hidden', 'num_topk', 'num_experts',
        'data_size_mb', 'total_time_us', 'total_throughput_gbps',
        'dispatch_sms', 'dispatch_nvl_chunk', 'dispatch_rdma_chunk',
        'dispatch_transmit_us', 'dispatch_notify_us',
        'dispatch_rdma_bandwidth_gbps', 'dispatch_nvl_bandwidth_gbps',
        'combine_sms', 'combine_nvl_chunk', 'combine_rdma_chunk',
        'combine_transmit_us', 'combine_notify_us',
        'combine_rdma_bandwidth_gbps', 'combine_nvl_bandwidth_gbps',
        'avg_rdma_bandwidth_gbps', 'avg_nvl_bandwidth_gbps',
        'total_transmit_time_us', 'total_notify_time_us',
    ]
    all_columns = set()
    for data in all_data:
        all_columns.update(data.keys())
    sorted_columns: List[str] = []
    for col in priority_columns:
        if col in all_columns:
            sorted_columns.append(col)
            all_columns.remove(col)
    sorted_columns.extend(sorted(all_columns))

    wb = Workbook()
    ws = wb.active
    ws.title = 'åŸå§‹æ•°æ®'
    ws.append(sorted_columns)

    sorted_data = sorted(all_data, key=lambda x: x.get('num_tokens', 0))
    for row in sorted_data:
        excel_row = []
        for col in sorted_columns:
            value = row.get(col, '')
            if isinstance(value, float):
                value = round(value, 4)
            excel_row.append(value)
        ws.append(excel_row)

    output_dir = os.path.dirname(os.path.abspath(output_path)) or '.'
    os.makedirs(output_dir, exist_ok=True)
    wb.save(output_path)
    print(f"âœ… ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    return True

def try_create_excel_report_multi_sheet(logfile_to_data: Dict[str, List[Dict]], output_path: str) -> bool:
    """
    ç”Ÿæˆå¤šsheetçš„Excelï¼šæ¯ä¸ªæ—¥å¿—æ–‡ä»¶å¯¹åº”ä¸€ä¸ªsheetï¼Œsheetåä¸ºæ—¥å¿—æ–‡ä»¶åã€‚
    ä¼˜å…ˆä½¿ç”¨pandasï¼›è‹¥ä¸å¯ç”¨ï¼Œé€€å›openpyxlã€‚
    """
    # è¿‡æ»¤ç©ºæ•°æ®
    logfile_to_data = {k: v for k, v in logfile_to_data.items() if v}
    if not logfile_to_data:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
        return False

    # å°è¯•pandas
    try:
        from openpyxl import Workbook  # type: ignore
    except ImportError:
        print("âŒ æœªå®‰è£…openpyxlï¼Œæ— æ³•ç”ŸæˆExcelã€‚è¯·å®‰è£…åé‡è¯•: pip install openpyxl")
        return False

    wb = Workbook()
    # é»˜è®¤å·¥ä½œè¡¨å°†ç”¨äºç¬¬ä¸€ä¸ªsheet
    first = True
    used_names = set()
    for path, rows in logfile_to_data.items():
        base = os.path.basename(path)
        name = _sanitize_sheet_name(base)
        original = name
        idx = 1
        while name in used_names:
            suffix = f"_{idx}"
            name = _sanitize_sheet_name((original[:31 - len(suffix)]) + suffix)
            idx += 1
        used_names.add(name)

        if first:
            ws = wb.active
            ws.title = name
            first = False
        else:
            ws = wb.create_sheet(title=name)

        # è®¡ç®—åˆ—é›†åˆï¼Œä¼˜å…ˆé‡è¦åˆ—
        priority_columns = [
            'num_tokens', 'hidden', 'num_topk', 'num_experts',
            'data_size_mb', 'total_time_us', 'total_throughput_gbps',
            'dispatch_sms', 'dispatch_nvl_chunk', 'dispatch_rdma_chunk',
            'dispatch_transmit_us', 'dispatch_notify_us',
            'dispatch_rdma_bandwidth_gbps', 'dispatch_nvl_bandwidth_gbps',
            'combine_sms', 'combine_nvl_chunk', 'combine_rdma_chunk',
            'combine_transmit_us', 'combine_notify_us',
            'combine_rdma_bandwidth_gbps', 'combine_nvl_bandwidth_gbps',
            'avg_rdma_bandwidth_gbps', 'avg_nvl_bandwidth_gbps',
            'total_transmit_time_us', 'total_notify_time_us',
        ]
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        ordered = []
        for c in priority_columns:
            if c in all_columns:
                ordered.append(c)
                all_columns.remove(c)
        ordered.extend(sorted(all_columns))

        # è¡¨å¤´
        ws.append(ordered)

        # æŒ‰num_tokensæ’åºå†™å…¥
        rows_sorted = sorted(rows, key=lambda x: x.get('num_tokens', 0))
        for r in rows_sorted:
            row_vals = []
            for c in ordered:
                v = r.get(c, '')
                if isinstance(v, float):
                    v = round(v, 4)
                row_vals.append(v)
            ws.append(row_vals)

    output_dir = os.path.dirname(os.path.abspath(output_path)) or '.'
    os.makedirs(output_dir, exist_ok=True)
    wb.save(output_path)
    print(f"âœ… ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    return True

def main():
    parser = argparse.ArgumentParser(description='ä»ç›®å½•ä¸‹æ‰€æœ‰.logç”Ÿæˆå¤šSheetçš„ExcelæŠ¥å‘Š')
    parser.add_argument('--log-dir', default='/home/xutingz/workspace/bench/deepep/v1',
                       help='åŒ…å«.logæ–‡ä»¶çš„ç›®å½•è·¯å¾„ (é»˜è®¤: /home/xutingz/workspace/bench/deepep)')
    parser.add_argument('--output-excel', default='./internode_performance.xlsx',
                       help='è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./internode_performance.xlsx)')
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹è§£æDeepEP 16èŠ‚ç‚¹è·¨èŠ‚ç‚¹æµ‹è¯•æ—¥å¿—...")
    print(f"æ—¥å¿—ç›®å½•: {args.log_dir}")
    print(f"Excelè¾“å‡º: {args.output_excel}")
    print("=" * 50)
    
    # æ”¶é›†æ—¥å¿—æ–‡ä»¶
    log_files = collect_log_files(args.log_dir)
    if not log_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .log æ—¥å¿—æ–‡ä»¶")
        return
    print(f"æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œå¼€å§‹è§£æ...")

    logfile_to_data: Dict[str, List[Dict]] = {}
    for log_file in log_files:
        print(f"æ­£åœ¨è§£æ: {os.path.basename(log_file)}...")
        if log_file.endswith('ll.log'):
            data = parse_ll_log_file(log_file)
        else:
            data = parse_log_file(log_file)
        if data:
            logfile_to_data[log_file] = data

    if not logfile_to_data:
        print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
        return

    # ç”Ÿæˆå¤šsheet Excel
    print("\nğŸ“Š å°è¯•ç”Ÿæˆå¤šSheet ExcelæŠ¥å‘Š...")
    excel_success = try_create_excel_report_multi_sheet(logfile_to_data, args.output_excel)
    
    # æ±‡æ€»æ‰“å°ï¼ˆè·¨æ‰€æœ‰æ–‡ä»¶ï¼‰
    all_data: List[Dict] = []
    for rows in logfile_to_data.values():
        all_data.extend(rows)
    if all_data:
        print(f"\nğŸ“ˆ æ•°æ®æ±‡æ€»:")
        print(f"  - Tokenæ•°èŒƒå›´: {min(d.get('num_tokens', 0) for d in all_data)} - {max(d.get('num_tokens', 0) for d in all_data)}")
        if 'num_experts' in all_data[0]:
            print(f"  - ä¸“å®¶æ•°: {all_data[0].get('num_experts', 'N/A')}")
        avg_rdma_bw = [d.get('avg_rdma_bandwidth_gbps', 0) for d in all_data if 'avg_rdma_bandwidth_gbps' in d]
        avg_nvl_bw = [d.get('avg_nvl_bandwidth_gbps', 0) for d in all_data if 'avg_nvl_bandwidth_gbps' in d]
        if avg_rdma_bw:
            print(f"  - å¹³å‡RDMAå¸¦å®½: {sum(avg_rdma_bw)/len(avg_rdma_bw):.2f} GB/s")
        if avg_nvl_bw:
            print(f"  - å¹³å‡NVLå¸¦å®½: {sum(avg_nvl_bw)/len(avg_nvl_bw):.2f} GB/s")
        total_throughput = [d.get('total_throughput_gbps', 0) for d in all_data if 'total_throughput_gbps' in d]
        if total_throughput:
            print(f"  - æ•´ä½“å¹³å‡ååç‡: {sum(total_throughput)/len(total_throughput):.2f} GB/s")
    
    print(f"\nğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    if excel_success:
        print(f"  - Excel: {args.output_excel}")

if __name__ == '__main__':
    main()
